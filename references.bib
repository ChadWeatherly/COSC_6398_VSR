
@inproceedings{rombach_high-resolution_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {High-Resolution Image Synthesis with Latent Diffusion Models},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878449/},
	doi = {10.1109/CVPR52688.2022.01042},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models ({DMs}) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful {DMs} often consumes hundreds of {GPU} days and inference is expensive due to sequential evaluations. To enable {DM} training on limited computational resources while retaining their quality and ﬂexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the ﬁrst time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual ﬁdelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and ﬂexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models ({LDMs}) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while signiﬁcantly reducing computational requirements compared to pixel-based {DMs}.},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {10674--10685},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
	urldate = {2024-03-22},
	date = {2022-06},
	langid = {english},
}

@article{nah_ntire_2021,
	title = {{NTIRE} 2021 Challenge on Image Deblurring},
	abstract = {Motion blur is a common photography artifact in dynamic environments that typically comes jointly with the other types of degradation. This paper reviews the {NTIRE} 2021 Challenge on Image Deblurring. In this challenge report, we describe the challenge speciﬁcs and the evaluation results from the 2 competition tracks with the proposed solutions. While both the tracks aim to recover a highquality clean image from a blurry image, different artifacts are jointly involved. In track 1, the blurry images are in a low resolution while track 2 images are compressed in {JPEG} format. In each competition, there were 338 and 238 registered participants and in the ﬁnal testing phase, 18 and 17 teams competed. The winning methods demonstrate the state-of-the-art performance on the image deblurring task with the jointly combined artifacts.},
	author = {Nah, Seungjun and Son, Sanghyun and Lee, Suyoung and Timofte, Radu and Lee, Kyoung Mu},
	date = {2021},
	langid = {english},
}

@article{dai_deformable_2016,
	title = {Deformable Convolutional Networks},
	abstract = {Convolutional neural networks ({CNNs}) are inherently limited to model geometric transformations due to the ﬁxed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of {CNNs}, namely, deformable convolution and deformable {RoI} pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing {CNNs} and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the ﬁrst time, we show that learning dense spatial transformation in deep {CNNs} is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/ msracver/Deformable-{ConvNets}.},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	date = {2016},
	langid = {english},
}

@online{xue_video_2019,
	title = {Video Enhancement with Task-Oriented Flow},
	url = {http://toflow.csail.mit.edu/},
	author = {Xue, Tianfan and Chen, Baian and Wu, Jiajun and Wei, Donglai and Freeman, William T.},
	urldate = {2024-02-17},
	date = {2019},
}

@online{rota_enhancing_2023,
	title = {Enhancing Perceptual Quality in Video Super-Resolution through Temporally-Consistent Detail Synthesis using Diffusion Models},
	url = {https://ar5iv.labs.arxiv.org/html/2311.15908},
	titleaddon = {ar5iv},
	author = {Rota, Claudio and {Marco Buzzelli} and van de Weijer, Joost},
	urldate = {2024-02-01},
	date = {2023},
	langid = {english},
}

@online{noauthor_video_nodate,
	title = {Video generation models as world simulators},
	url = {https://openai.com/research/video-generation-models-as-world-simulators},
	abstract = {We explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.},
	urldate = {2024-02-19},
	langid = {american},
}

@inproceedings{peebles_scalable_2023,
	location = {Paris, France},
	title = {Scalable Diffusion Models with Transformers},
	isbn = {9798350307184},
	url = {https://ieeexplore.ieee.org/document/10377858/},
	doi = {10.1109/ICCV51070.2023.00387},
	eventtitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	pages = {4172--4182},
	booktitle = {2023 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Peebles, William and Xie, Saining},
	urldate = {2024-02-19},
	date = {2023-10-01},
	langid = {english},
}

@article{liang_recurrent_nodate,
	title = {Recurrent Video Restoration Transformer with Guided Deformable Attention},
	abstract = {Video restoration aims at restoring multiple high-quality frames from multiple lowquality frames. Existing video restoration methods generally fall into two extreme cases, i.e., they either restore all frames in parallel or restore the video frame by frame in a recurrent way, which would result in different merits and drawbacks. Typically, the former has the advantage of temporal information fusion. However, it suffers from large model size and intensive memory consumption; the latter has a relatively small model size as it shares parameters across frames; however, it lacks long-range dependency modeling ability and parallelizability. In this paper, we attempt to integrate the advantages of the two cases by proposing a recurrent video restoration transformer, namely {RVRT}. {RVRT} processes local neighboring frames in parallel within a globally recurrent framework which can achieve a good trade-off between model size, effectiveness, and efficiency. Specifically, {RVRT} divides the video into multiple clips and uses the previously inferred clip feature to estimate the subsequent clip feature. Within each clip, different frame features are jointly updated with implicit feature aggregation. Across different clips, the guided deformable attention is designed for clip-to-clip alignment, which predicts multiple relevant locations from the whole inferred clip and aggregates their features by the attention mechanism. Extensive experiments on video super-resolution, deblurring, and denoising show that the proposed {RVRT} achieves state-of-the-art performance on benchmark datasets with balanced model size, testing memory and runtime. The codes are available at https://github.com/{JingyunLiang}/{RVRT}.},
	author = {Liang, Jingyun and Fan, Yuchen and Xiang, Xiaoyu and Ranjan, Rakesh and Ilg, Eddy and Green, Simon and Cao, Jiezhang and Zhang, Kai and Timofte, Radu and Gool, Luc Van},
	langid = {english},
}

@inproceedings{rombach_high-resolution_2022-1,
	location = {New Orleans, {LA}, {USA}},
	title = {High-Resolution Image Synthesis with Latent Diffusion Models},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878449/},
	doi = {10.1109/CVPR52688.2022.01042},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models ({DMs}) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful {DMs} often consumes hundreds of {GPU} days and inference is expensive due to sequential evaluations. To enable {DM} training on limited computational resources while retaining their quality and ﬂexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the ﬁrst time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual ﬁdelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and ﬂexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models ({LDMs}) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while signiﬁcantly reducing computational requirements compared to pixel-based {DMs}.},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {10674--10685},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
	urldate = {2024-02-05},
	date = {2022-06},
	langid = {english},
}

@inproceedings{chan_investigating_2022,
	location = {New Orleans, {LA}, {USA}},
	title = {Investigating Tradeoffs in Real-World Video Super-Resolution},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880338/},
	doi = {10.1109/CVPR52688.2022.00587},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {5952--5961},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Chan, Kelvin C.K. and Zhou, Shangchen and Xu, Xiangyu and Loy, Chen Change},
	urldate = {2024-02-05},
	date = {2022-06},
	langid = {english},
}

@inproceedings{chan_basicvsr_2021,
	location = {Nashville, {TN}, {USA}},
	title = {{BasicVSR}: The Search for Essential Components in Video Super-Resolution and Beyond},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577681/},
	doi = {10.1109/CVPR46437.2021.00491},
	shorttitle = {{BasicVSR}},
	abstract = {Video super-resolution ({VSR}) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for {VSR} guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, {BasicVSR}, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of {BasicVSR} by presenting an informationreﬁll mechanism and a coupled propagation scheme to facilitate information aggregation. The {BasicVSR} and its extension, {IconVSR}, can serve as strong baselines for future {VSR} approaches.},
	eventtitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {4945--4954},
	booktitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Chan, Kelvin C.K. and Wang, Xintao and Yu, Ke and Dong, Chao and Loy, Chen Change},
	urldate = {2024-02-02},
	date = {2021-06},
	langid = {english},
}

@inproceedings{zhang_unreasonable_2018,
	location = {Salt Lake City, {UT}},
	title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578166/},
	doi = {10.1109/CVPR.2018.00068},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as {PSNR} and {SSIM}, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the {VGG} network trained on {ImageNet} classiﬁcation has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called “perceptual losses”? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We ﬁnd that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to {ImageNet}-trained {VGG} features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {586--595},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	urldate = {2024-02-01},
	date = {2018-06},
	langid = {english},
}

@inproceedings{johnson_perceptual_2016,
	location = {Cham},
	title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
	isbn = {978-3-319-46475-6},
	doi = {10.1007/978-3-319-46475-6_43},
	series = {Lecture Notes in Computer Science},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	pages = {694--711},
	booktitle = {Computer Vision – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	date = {2016},
	langid = {english},
	keywords = {Deep learning, Style transfer, Super-resolution},
}

@inproceedings{chan_basicvsr_2022,
	title = {{BasicVSR}++: Improving Video Super-Resolution With Enhanced Propagation and Alignment},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.html},
	shorttitle = {{BasicVSR}++},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {5972--5981},
	author = {Chan, Kelvin C. K. and Zhou, Shangchen and Xu, Xiangyu and Loy, Chen Change},
	urldate = {2024-01-29},
	date = {2022},
	langid = {english},
}

@article{shi_rethinking_nodate,
	title = {Rethinking Alignment in Video Super-Resolution Transformers},
	abstract = {The alignment of adjacent frames is considered an essential operation in video super-resolution ({VSR}). Advanced {VSR} models, including the latest {VSR} Transformers, are generally equipped with well-designed alignment modules. However, the progress of the self-attention mechanism may violate this common sense. In this paper, we rethink the role of alignment in {VSR} Transformers and make several counter-intuitive observations. Our experiments show that: (i) {VSR} Transformers can directly utilize multi-frame information from unaligned videos, and (ii) existing alignment methods are sometimes harmful to {VSR} Transformers. These observations indicate that we can further improve the performance of {VSR} Transformers simply by removing the alignment module and adopting a larger attention window. Nevertheless, such designs will dramatically increase the computational burden, and cannot deal with large motions. Therefore, we propose a new and efficient alignment method called patch alignment, which aligns image patches instead of pixels. {VSR} Transformers equipped with patch alignment could demonstrate state-of-the-art performance on multiple benchmarks. Our work provides valuable insights on how multi-frame information is used in {VSR} and how to select alignment methods for different networks/datasets. Codes and models will be released at https://github.com/{XPixelGroup}/{RethinkVSRAlignment}.},
	author = {Shi, Shuwei and Gu, Jinjin and Xie, Liangbin and Wang, Xintao and Yang, Yujiu and Dong, Chao},
	langid = {english},
}

@article{liu_video_2022,
	title = {Video super-resolution based on deep learning: a comprehensive survey},
	volume = {55},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10147-y},
	doi = {10.1007/s10462-022-10147-y},
	shorttitle = {Video super-resolution based on deep learning},
	abstract = {Video super-resolution ({VSR}) is reconstructing high-resolution videos from low resolution ones. Recently, the {VSR} methods based on deep neural networks have made great progress. However, there is rarely systematical review on these methods. In this survey, we comprehensively investigate 37 state-of-the-art {VSR} methods based on deep learning. It is well known that the leverage of information contained in video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into seven sub-categories according to the ways of utilizing inter-frame information. Moreover, descriptions on the architecture design and implementation details are also included. Finally, we summarize and compare the performance of the representative {VSR} methods on some benchmark datasets. We also discuss the applications, and some challenges, which need to be further addressed by researchers in the community of {VSR}. To the best of our knowledge, this work is the first systematic review on {VSR} tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding of the {VSR} techniques based on deep learning.},
	pages = {5981--6035},
	number = {8},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Liu, Hongying and Ruan, Zhubo and Zhao, Peng and Dong, Chao and Shang, Fanhua and Liu, Yuanyuan and Yang, Linlin and Timofte, Radu},
	urldate = {2023-12-19},
	date = {2022-12-01},
	langid = {english},
	keywords = {Convolutional neural networks, Deep learning, Inter-frame information, Video super-resolution},
}

@online{noauthor_fast_nodate,
	title = {The fast Fourier transform and its applications},
	url = {https://dl.acm.org/doi/abs/10.5555/47314},
	titleaddon = {Guide books},
	urldate = {2023-04-26},
	doi = {10.5555/47314},
	note = {Archive Location: world},
}

@article{watson_deep_nodate,
	title = {Deep Learning Techniques for Super-Resolution in Video Games},
	abstract = {The computational cost of video game graphics is increasing and hardware for processing graphics is struggling to keep up. This means that computer scientists need to develop creative new ways to improve the performance of graphical processing hardware. Deep learning techniques for video super-resolution can enable video games to have high quality graphics whilst offsetting much of the computational cost. These emerging technologies allow consumers to have improved performance and enjoyment from video games and have the potential to become standard within the game development industry.},
	author = {Watson, Alexander},
	langid = {english},
}

@online{noauthor_deep_2018,
	title = {Deep Learning Super Sampling ({DLSS})},
	url = {https://developer.nvidia.com/rtx/dlss},
	abstract = {Boosts frame rates and generates sharp images.},
	titleaddon = {{NVIDIA} Developer},
	urldate = {2023-04-26},
	date = {2018-09-18},
	langid = {american},
}

@article{sonderby_ladder_nodate,
	title = {Ladder Variational Autoencoders},
	abstract = {Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difﬁcult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch-normalization and deterministic warm-up (gradually turning on the {KL}-term) are crucial for training variational models with many stochastic layers.},
	author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
	langid = {english},
}

@article{kingma_introduction_2019,
	title = {An Introduction to Variational Autoencoders},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-056},
	doi = {10.1561/2200000056},
	abstract = {An Introduction to Variational Autoencoders},
	pages = {307--392},
	number = {4},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{MAL}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2023-04-22},
	date = {2019-11-27},
	note = {Publisher: Now Publishers, Inc.},
}

@article{prost_diverse_nodate,
	title = {Diverse super-resolution with pretrained hierarchical variational autoencoders},
	abstract = {Image super-resolution is a one-to-many problem, but most deep-learning based methods only provide one single solution to this problem. In this work, we tackle the problem of diverse super-resolution by reusing {VD}-{VAE}, a state-of-the art variational autoencoder ({VAE}). We ﬁnd that the hierarchical latent representation learned by {VD}-{VAE} naturally separates the image low-frequency information, encoded in the latent groups at the top of the hierarchy, from the image high-frequency details, determined by the latent groups at the bottom of the latent hierarchy. Starting from this observation, we design a super-resolution model exploiting the speciﬁc structure of {VD}-{VAE} latent space. Speciﬁcally, we train an encoder to encode low-resolution images in the subset of {VD}-{VAE} latent space encoding the low-frequency information, and we combine this encoder with {VD}-{VAE} generative model to sample diverse super-resolved version of a low-resolution input. We demonstrate the ability of our method to generate diverse solutions to the super-resolution problem on face super-resolution with upsampling factors ×4, ×8 and ×16.},
	author = {Prost, Jean and Houdard, Antoine and Almansa, Andrés and Papadakis, Nicolas},
	langid = {english},
}

@incollection{vedaldi_varsr_2020,
	location = {Cham},
	title = {{VarSR}: Variational Super-Resolution Network for Very Low Resolution Images},
	volume = {12368},
	isbn = {978-3-030-58591-4 978-3-030-58592-1},
	url = {https://link.springer.com/10.1007/978-3-030-58592-1_26},
	shorttitle = {{VarSR}},
	abstract = {As is well known, single image super-resolution ({SR}) is an ill-posed problem where multiple high resolution ({HR}) images can be matched to one low resolution ({LR}) image due to the diﬀerence in their representation capabilities. Such many-to-one nature is particularly magniﬁed when super-resolving with large upscaling factors from very low dimensional domains such as 8×8 resolution where detailed information of {HR} is hardly discovered. Most existing methods are optimized for deterministic generation of {SR} images under pre-deﬁned objectives such as pixel-level reconstruction and thus limited to the one-to-one correspondence between {LR} and {SR} images against the nature. In this paper, we propose {VarSR}, Variational Super Resolution Network, that matches latent distributions of {LR} and {HR} images to recover the missing details. Speciﬁcally, we draw samples from the learned common latent distribution of {LR} and {HR} to generate diverse {SR} images as the many-to-one relationship. Experimental results validate that our method can produce more accurate and perceptually plausible {SR} images from very low resolutions compared to the deterministic techniques.},
	pages = {431--447},
	booktitle = {Computer Vision – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Hyun, Sangeek and Heo, Jae-Pil},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	urldate = {2023-04-22},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-58592-1_26},
	note = {Series Title: Lecture Notes in Computer Science},
}

@misc{child_very_2021,
	title = {Very Deep {VAEs} Generalize Autoregressive Models and Can Outperform Them on Images},
	url = {http://arxiv.org/abs/2011.10650},
	abstract = {We present a hierarchical {VAE} that, for the ﬁrst time, generates samples quickly and outperforms the {PixelCNN} in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, {VAEs} can actually represent autoregressive models, as well as faster, better models if they exist, when made sufﬁciently deep. Despite this, autoregressive models have historically outperformed {VAEs} in loglikelihood. We test if insufﬁcient depth explains why by scaling a {VAE} to greater stochastic depth than previously explored and evaluating it {CIFAR}-10, {ImageNet}, and {FFHQ}. In comparison to the {PixelCNN}, these very deep {VAEs} achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the {VAE} learns efﬁcient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
	number = {{arXiv}:2011.10650},
	publisher = {{arXiv}},
	author = {Child, Rewon},
	urldate = {2023-04-22},
	date = {2021-03-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2011.10650 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{liu_variational_2021,
	location = {Nashville, {TN}, {USA}},
	title = {Variational {AutoEncoder} for Reference based Image Super-Resolution},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9522932/},
	doi = {10.1109/CVPRW53098.2021.00063},
	abstract = {In this paper, we propose a novel reference based image super-resolution approach via Variational {AutoEncoder} ({RefVAE}). Existing state-of-the-art methods mainly focus on single image super-resolution which cannot perform well on large upsampling factors, e.g., 8×. We propose a reference based image super-resolution, for which any arbitrary image can act as a reference for super-resolution. Even using random map or low-resolution image itself, the proposed {RefVAE} can transfer the knowledge from the reference to the super-resolved images. Depending upon different references, the proposed method can generate diﬀerent versions of super-resolved images from a hidden superresolution space. Besides using diﬀerent datasets for some standard evaluations with {PSNR} and {SSIM}, we also took part in the {NTIRE}2021 {SR} Space challenge [29] and have provided results of the randomness evaluation of our approach. Compared to other state-of-the-art methods, our approach achieves higher diverse scores.},
	eventtitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	pages = {516--525},
	booktitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	publisher = {{IEEE}},
	author = {Liu, Zhi-Song and Siu, Wan-Chi and Wang, Li-Wen},
	urldate = {2023-04-21},
	date = {2021-06},
	langid = {english},
}

@inproceedings{chira_image_2023,
	location = {Cham},
	title = {Image Super-Resolution with Deep Variational Autoencoders},
	isbn = {978-3-031-25063-7},
	doi = {10.1007/978-3-031-25063-7_24},
	series = {Lecture Notes in Computer Science},
	abstract = {Image super-resolution ({SR}) techniques are used to generate a high-resolution image from a low-resolution image. Until now, deep generative models such as autoregressive models and Generative Adversarial Networks ({GANs}) have proven to be effective at modelling high-resolution images. {VAE}-based models have often been criticised for their feeble generative performance, but with new advancements such as {VDVAE}, there is now strong evidence that deep {VAEs} have the potential to outperform current state-of-the-art models for high-resolution image generation. In this paper, we introduce {VDVAE}-{SR}, a new model that aims to exploit the most recent deep {VAE} methodologies to improve upon the results of similar models. {VDVAE}-{SR} tackles image super-resolution using transfer learning on pretrained {VDVAEs}. The presented model is competitive with other state-of-the-art models, having comparable results on image quality metrics.},
	pages = {395--411},
	booktitle = {Computer Vision – {ECCV} 2022 Workshops},
	publisher = {Springer Nature Switzerland},
	author = {Chira, Darius and Haralampiev, Ilian and Winther, Ole and Dittadi, Andrea and Liévin, Valentin},
	editor = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
	date = {2023},
	langid = {english},
	keywords = {Deep variational autoencoders, {SR}, Single-image super-resolution, Transfer learning, {VDVAE}},
}

@online{noauthor_driving_nodate,
	title = {Driving in Severe Weather {\textbar} {NHTSA}},
	url = {https://www.nhtsa.gov/road-safety/driving-in-severe-weather},
	type = {Text},
	urldate = {2022-12-09},
	langid = {english},
}

@inproceedings{yuan_hetero-convlstm_2018,
	location = {London United Kingdom},
	title = {Hetero-{ConvLSTM}: A Deep Learning Approach to Traffic Accident Prediction on Heterogeneous Spatio-Temporal Data},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3219922},
	doi = {10.1145/3219819.3219922},
	shorttitle = {Hetero-{ConvLSTM}},
	abstract = {Predicting traffic accidents is a crucial problem to improving transportation and public safety as well as safe routing. The problem is also challenging due to the rareness of accidents in space and time and spatial heterogeneity of the environment (e.g., urban vs. rural). Most previous research on traffic accident prediction conducted by domain researchers simply applied classical prediction models on limited data without addressing the above challenges properly, thus leading to unsatisfactory performance. A small number of recent works have attempted to use deep learning for traffic accident prediction. However, they either ignore time information or use only data from a small and homogeneous study area (a city), without handling spatial heterogeneity and temporal auto-correlation properly at the same time. In this paper we perform a comprehensive study on the traffic accident prediction problem using the Convolutional Long {ShortTerm} Memory ({ConvLSTM}) neural network model. A number of detailed features such as weather, environment, road condition, and traffic volume are extracted from big datasets over the state of Iowa across 8 years. To address the spatial heterogeneity challenge in the data, we propose a Hetero-{ConvLSTM} framework, where a few novel ideas are implemented on top of the basic {ConvLSTM} model, such as incorporating spatial graph features and spatial model ensemble. Extensive experiments on the 8-year data over the entire state of Iowa show that the proposed framework makes reasonably accurate predictions and significantly improves the prediction accuracy over baseline approaches.},
	eventtitle = {{KDD} '18: The 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {984--992},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Yuan, Zhuoning and Zhou, Xun and Yang, Tianbao},
	urldate = {2022-12-08},
	date = {2018-07-19},
	langid = {english},
}

@inproceedings{yuan_hetero-convlstm_2018-1,
	location = {London United Kingdom},
	title = {Hetero-{ConvLSTM}: A Deep Learning Approach to Traffic Accident Prediction on Heterogeneous Spatio-Temporal Data},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3219922},
	doi = {10.1145/3219819.3219922},
	shorttitle = {Hetero-{ConvLSTM}},
	abstract = {Predicting traffic accidents is a crucial problem to improving transportation and public safety as well as safe routing. The problem is also challenging due to the rareness of accidents in space and time and spatial heterogeneity of the environment (e.g., urban vs. rural). Most previous research on traffic accident prediction conducted by domain researchers simply applied classical prediction models on limited data without addressing the above challenges properly, thus leading to unsatisfactory performance. A small number of recent works have attempted to use deep learning for traffic accident prediction. However, they either ignore time information or use only data from a small and homogeneous study area (a city), without handling spatial heterogeneity and temporal auto-correlation properly at the same time. In this paper we perform a comprehensive study on the traffic accident prediction problem using the Convolutional Long {ShortTerm} Memory ({ConvLSTM}) neural network model. A number of detailed features such as weather, environment, road condition, and traffic volume are extracted from big datasets over the state of Iowa across 8 years. To address the spatial heterogeneity challenge in the data, we propose a Hetero-{ConvLSTM} framework, where a few novel ideas are implemented on top of the basic {ConvLSTM} model, such as incorporating spatial graph features and spatial model ensemble. Extensive experiments on the 8-year data over the entire state of Iowa show that the proposed framework makes reasonably accurate predictions and significantly improves the prediction accuracy over baseline approaches.},
	eventtitle = {{KDD} '18: The 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {984--992},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Yuan, Zhuoning and Zhou, Xun and Yang, Tianbao},
	urldate = {2022-12-08},
	date = {2018-07-19},
	langid = {english},
}

@online{noauthor_hetero-convlstm_nodate,
	title = {Hetero-{ConvLSTM} {\textbar} Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	url = {https://dl.acm.org/doi/10.1145/3219819.3219922},
	urldate = {2022-12-08},
}

@online{noauthor_kdd_nodate,
	title = {{KDD} 2018 {\textbar} Hetero-{ConvLSTM}: A Deep Learning Approach to Traffic Accident Prediction on Heterogeneous Spatio-Tem},
	url = {https://www.kdd.org/kdd2018/accepted-papers/view/hetero-convlstm-a-deep-learning-approach-to-traffic-accident-prediction-on-},
	urldate = {2022-12-08},
}

@article{najjar_combining_2017,
	title = {Combining Satellite Imagery and Open Data to Map Road Safety},
	volume = {31},
	rights = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11168},
	doi = {10.1609/aaai.v31i1.11168},
	abstract = {Improving road safety is critical for the sustainable development of cities. A road safety map is a powerful tool that can help prevent future traffic accidents. However, accurate mapping requires accurate data collection, which is both expensive and labor intensive. Satellite imagery is increasingly becoming abundant, higher in resolution and affordable. Given the recent successes deep learning has achieved in the visual recognition field, we are interested in investigating whether it is possible to use deep learning to accurately predict road safety directly from raw satellite imagery. To this end, we propose a deep learning-based mapping approach that leverages open data to learn from raw satellite imagery robust deep models able to predict accurate city-scale road safety maps at an affordable cost. To empirically validate the proposed approach, we trained a deep model on satellite images obtained from over 647 thousand traffic-accident reports collected over a period of four years by the New York city Police Department. The best model predicted road safety from raw satellite imagery with an accuracy of 78\%. We also used the New York city model to predict for the city of Denver a city-scale map indicating road safety in three levels. Compared to a map made from three years' worth of data collected by the Denver city Police Department, the map predicted from raw satellite imagery has an accuracy of 73\%.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Najjar, Alameen and Kaneko, Shun’ichi and Miyanaga, Yoshikazu},
	urldate = {2022-12-08},
	date = {2017-02-12},
	langid = {english},
	note = {Number: 1},
	keywords = {Convolutional Neural Networks},
}

@inproceedings{mohammed_cars2021_2022,
	title = {{CARS}2021 - Prediction of Traffic Accidents Using An Artificial Neural Network},
	author = {Mohammed, Ali and Shakir, Alaa},
	date = {2022-02-04},
}

@online{noauthor_model_nodate,
	title = {A model of traffic accident prediction based on convolutional neural network {\textbar} Semantic Scholar},
	url = {https://www.semanticscholar.org/paper/A-model-of-traffic-accident-prediction-based-on-Wenqi-Dong-yu/eb9d415ded31552e4eae6c9b64ab12c9a2a67058},
	urldate = {2022-12-08},
}

@article{lin_novel_2015,
	title = {A novel variable selection method based on frequent pattern tree for real-time traffic accident risk prediction},
	volume = {55},
	issn = {0968090X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0968090X15000947},
	doi = {10.1016/j.trc.2015.03.015},
	abstract = {Traffic accident data are usually noisy, contain missing values, and heterogeneous. How to select the most important variables to improve real-time traffic accident risk prediction has become a concern of many recent studies. This paper proposes a novel variable selection method based on the Frequent Pattern tree ({FP} tree) algorithm. First, all the frequent patterns in the traffic accident dataset are discovered. Then for each frequent pattern, a new criterion, called the Relative Object Purity Ratio ({ROPR}) which we proposed, is calculated. This {ROPR} is added to the importance score of the variables that differentiates one frequent pattern from the others. To test the proposed method, a dataset was compiled from the traffic accidents records detected by only one detector on interstate highway I-64 in Virginia in 2005. This data set was then linked to other variables such as real-time traffic information and weather conditions. Both the proposed method based on the {FP} tree algorithm, as well as the widely utilized, random forest method, were then used to identify the important variables or the Virginia data set. The results indicate that there are some differences between the variables deemed important by the {FP} tree and those selected by the random forest method. Following this, two baseline models (i.e. a nearest neighbor (k-{NN}) method and a Bayesian network) were developed to predict accident risk based on the variables identified by both the {FP} tree method and the random forest method. The results show that the models based on the variable selection using the {FP} tree performed better than those based on the random forest method for several versions of the k-{NN} and Bayesian network models. The best results were derived from a Bayesian network model using variables from {FP} tree. That model could predict 61.11\% of accidents accurately, while having a false alarm rate of 38.16\%.},
	pages = {444--459},
	journaltitle = {Transportation Research Part C: Emerging Technologies},
	shortjournal = {Transportation Research Part C: Emerging Technologies},
	author = {Lin, Lei and Wang, Qian and Sadek, Adel W.},
	urldate = {2022-12-08},
	date = {2015-06},
	langid = {english},
}

@article{kumar_data_2015,
	title = {A data mining framework to analyze road accident data},
	volume = {2},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-015-0035-y},
	doi = {10.1186/s40537-015-0035-y},
	abstract = {One of the key objectives in accident data analysis to identify the main factors associated with a road and traffic accident. However, heterogeneous nature of road accident data makes the analysis task difficult. Data segmentation has been used widely to overcome this heterogeneity of the accident data. In this paper, we proposed a framework that used K-modes clustering technique as a preliminary task for segmentation of 11,574 road accidents on road network of Dehradun (India) between 2009 and 2014 (both included). Next, association rule mining are used to identify the various circumstances that are associated with the occurrence of an accident for both the entire data set ({EDS}) and the clusters identified by K-modes clustering algorithm. The findings of cluster based analysis and entire data set analysis are then compared. The results reveal that the combination of k mode clustering and association rule mining is very inspiring as it produces important information that would remain hidden if no segmentation has been performed prior to generate association rules. Further a trend analysis have also been performed for each clusters and {EDS} accidents which finds different trends in different cluster whereas a positive trend is shown by {EDS}. Trend analysis also shows that prior segmentation of accident data is very important before analysis.},
	pages = {26},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {Journal of Big Data},
	author = {Kumar, Sachin and Toshniwal, Durga},
	urldate = {2022-12-08},
	date = {2015-11-21},
	keywords = {Accident analysis, Clustering, Data mining, Road accidents},
}

@article{abellan_analysis_2013,
	title = {Analysis of traffic accident severity using Decision Rules via Decision Trees},
	volume = {40},
	issn = {0957-4174},
	url = {https://doi.org/10.1016/j.eswa.2013.05.027},
	doi = {10.1016/j.eswa.2013.05.027},
	abstract = {A Decision Tree ({DT}) is a potential method for studying traffic accident severity. One of its main advantages is that Decision Rules ({DRs}) can be extracted from its structure. And these {DRs} can be used to identify safety problems and establish certain measures of performance. However, when only one {DT} is used, rule extraction is limited to the structure of that {DT} and some important relationships between variables cannot be extracted. This paper presents a more effective method for extracting rules from {DTs}. The method's effectiveness when applied to a particular traffic accident dataset is shown. Specifically, our study focuses on traffic accident data from rural roads in Granada (Spain) from 2003 to 2009 (both included). The results show that we can obtain more than 70 relevant rules from our data using the new method, whereas with only one {DT} we would have extracted only five relevant rules from the same dataset.},
	pages = {6047--6054},
	number = {15},
	journaltitle = {Expert Systems with Applications: An International Journal},
	shortjournal = {Expert Syst. Appl.},
	author = {{AbelláN}, {JoaquíN} and {LóPez}, Griselda and De {OñA}, Juan},
	urldate = {2022-12-08},
	date = {2013-11-01},
	keywords = {Decision Rules, Decision Trees, Road safety, Severity, Traffic accident},
}

@online{noauthor_state-specific_2020,
	title = {State-Specific Costs of Motor Vehicle Crash Deaths {\textbar} Transportation Safety {\textbar} Injury Center {\textbar} {CDC}},
	url = {https://www.cdc.gov/transportationsafety/statecosts/index.html},
	urldate = {2022-12-08},
	date = {2020-11-06},
	langid = {english},
}

@article{kochanek_mortality_2020,
	title = {Mortality in the United States, 2019},
	pages = {8},
	number = {395},
	author = {Kochanek, Kenneth D},
	date = {2020},
	langid = {english},
}

@article{united_states_department_of_transportation_bureau_of_transportation_statistics_national_2019,
	title = {National Transportation Statistics ({NTS})},
	url = {https://tinyurl.com/rosapntlbtsNTS},
	doi = {10.21949/1503663},
	author = {United States. Department of Transportation. Bureau of Transportation Statistics},
	urldate = {2022-12-08},
	date = {2019},
	langid = {english},
	note = {Publisher: Not Available},
}

@inproceedings{moosavi_accident_2019,
	title = {Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights},
	url = {http://arxiv.org/abs/1909.09638},
	doi = {10.1145/3347146.3359078},
	shorttitle = {Accident Risk Prediction based on Heterogeneous Sparse Data},
	abstract = {Reducing traffic accidents is an important public safety challenge, therefore, accident analysis and prediction has been a topic of much research over the past few decades. Using small-scale datasets with limited coverage, being dependent on extensive set of data, and being not applicable for real-time purposes are the important shortcomings of the existing studies. To address these challenges, we propose a new solution for real-time traffic accident prediction using easy-to-obtain, but sparse data. Our solution relies on a deep-neural-network model (which we have named {DAP}, for Deep Accident Prediction); which utilizes a variety of data attributes such as traffic events, weather data, points-of-interest, and time. {DAP} incorporates multiple components including a recurrent (for time-sensitive data), a fully connected (for time-insensitive data), and a trainable embedding component (to capture spatial heterogeneity). To fill the data gap, we have - through a comprehensive process of data collection, integration, and augmentation - created a large-scale publicly available database of accident information named {US}-Accidents. By employing the {US}-Accidents dataset and through an extensive set of experiments across several large cities, we have evaluated our proposal against several baselines. Our analysis and results show significant improvements to predict rare accident events. Further, we have shown the impact of traffic information, time, and points-of-interest data for real-time accident prediction.},
	pages = {33--42},
	booktitle = {Proceedings of the 27th {ACM} {SIGSPATIAL} International Conference on Advances in Geographic Information Systems},
	author = {Moosavi, Sobhan and Samavatian, Mohammad Hossein and Parthasarathy, Srinivasan and Teodorescu, Radu and Ramnath, Rajiv},
	urldate = {2022-12-08},
	date = {2019-11-05},
	eprinttype = {arxiv},
	eprint = {1909.09638 [cs, stat]},
	keywords = {Computer Science - Databases, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{moosavi_countrywide_2019,
	title = {A Countrywide Traffic Accident Dataset},
	url = {http://arxiv.org/abs/1906.05409},
	abstract = {Reducing traffic accidents is an important public safety challenge. However, the majority of studies on traffic accident analysis and prediction have used small-scale datasets with limited coverage, which limits their impact and applicability; and existing large-scale datasets are either private, old, or do not include important contextual information such as environmental stimuli (weather, pointsof-interest, etc.). In order to help the research community address these shortcomings we have - through a comprehensive process of data collection, integration, and augmentation - created a largescale publicly available database of accident information named {USAccidents}. {US}-Accidents currently contains data about 2.25 million instances of traffic accidents that took place within the contiguous United States, and over the last three years. Each accident record consists of a variety of intrinsic and contextual attributes such as location, time, natural language description, weather, period-of-day, and points-of-interest. We present this dataset in this paper, along with a wide range of insights gleaned from this dataset with respect to the spatiotemporal characteristics of accidents. The dataset is publicly available at https://smoosavi.org/datasets/us\_accidents.},
	number = {{arXiv}:1906.05409},
	publisher = {{arXiv}},
	author = {Moosavi, Sobhan and Samavatian, Mohammad Hossein and Parthasarathy, Srinivasan and Ramnath, Rajiv},
	urldate = {2022-12-08},
	date = {2019-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.05409 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Databases},
}

@online{noauthor_elsevier_nodate,
	title = {Elsevier Enhanced Reader},
	url = {https://reader.elsevier.com/reader/sd/pii/S0306437920300557?token=552C1EDD100BB6EDACAAB49F0E12D18C4662DE795B0009D909110EF84630618187A0827BBFCBA9D64715C4BF574D1DF9&originRegion=us-east-1&originCreation=20211203223604},
	urldate = {2021-12-03},
	langid = {english},
	doi = {10.1016/j.is.2020.101562},
}

@article{sarlis_sports_2020,
	title = {Sports analytics — Evaluation of basketball players and team performance},
	volume = {93},
	issn = {0306-4379},
	url = {https://www.sciencedirect.com/science/article/pii/S0306437920300557},
	doi = {10.1016/j.is.2020.101562},
	abstract = {Given the recent trend in Data Science ({DS}) and Sports Analytics, an opportunity has arisen for utilizing Machine Learning ({ML}) and Data Mining ({DM}) techniques in sports. This paper reviews background and advanced basketball metrics used in National Basketball Association ({NBA}) and Euroleague games. The purpose of this paper is to benchmark existing performance analytics used in the literature for evaluating teams and players. Basketball is a sport that requires full set enumeration of parameters in order to understand the game in depth and analyze the strategy and decisions by minimizing unpredictability. This research provides valuable information for team and player performance basketball analytics to be used for better understanding of the game. Furthermore, these analytics can be used for team composition, athlete career improvement and assessing how this could be materialized for future predictions. Hence, critical analysis of these metrics are valuable tools for domain experts and decision makers to understand the strengths and weaknesses in the game, to better evaluate opponent teams, to see how to optimize performance indicators, to use them for team and player forecasting and finally to make better choices for team composition.},
	pages = {101562},
	journaltitle = {Information Systems},
	shortjournal = {Information Systems},
	author = {Sarlis, Vangelis and Tjortjis, Christos},
	urldate = {2021-12-03},
	date = {2020-11-01},
	langid = {english},
	keywords = {Business intelligence, Data analysis, Machine learning, Sports Data Mining ({DM}), Sports analytics, Statistics},
}

@article{darwish_optimized_2020,
	title = {An optimized model based on convolutional neural networks and orthogonal learning particle swarm optimization algorithm for plant diseases diagnosis},
	volume = {52},
	issn = {22106502},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650219305462},
	doi = {10.1016/j.swevo.2019.100616},
	abstract = {The plant disease classiﬁcation based on using digital images is very challenging. In the last decade, machine learning techniques and plant images classiﬁcation tools such as deep learning can be used for recognizing, detecting and diagnosing plant diseases. Currently, deep learning technology has been used for plant disease detection and classiﬁcation. In this paper, an ensemble model of two pre-trained convolutional neural networks ({CNNs}) namely {VGG}16 and {VGG}19 have been developed for the task plant disease diagnosis by classifying the leaves images of healthy and unhealthy. In this context, {CNNs} are used due to its capability of overcoming the technical problems which are associated with the classiﬁcation problem of plant diseases. However, {CNNs} suffer from a great variety of hyperparameters with speciﬁc architectures which is considered as a challenge to identify manually the optimal hyperparameters. Therefore, orthogonal learning particle swarm optimization ({OLPSO}) algorithm is utilized in this paper to optimize a number of these hyperparameters by ﬁnding optimal values for these hyperparameters rather than using traditional methods such as the manual trial and error method. In this paper, to prevent {CNNs} from falling into the local minimum and to train efﬁciently, an exponentially decaying learning rate ({EDLR}) schema is used. In this paper, the problem of the imbalanced used dataset has been solved by using random minority oversampling and random majority undersampling methods, and some restrictions in terms of both the number and diversity of samples have been overcome. The obtained results of this work show that the accuracy of the proposed model is very competitive. The experimental results are compared with the performance of other pre-trained {CNN} models namely {InceptionV}3 and Xception, whose hyperparameters were selected using a non-evolutionary method. The comparison results demonstrated that the proposed diagnostic approach has achieved higher performance than the other models.},
	pages = {100616},
	journaltitle = {Swarm and Evolutionary Computation},
	shortjournal = {Swarm and Evolutionary Computation},
	author = {Darwish, Ashraf and Ezzat, Dalia and Hassanien, Aboul Ella},
	urldate = {2021-12-02},
	date = {2020-02},
	langid = {english},
}

@article{darwish_optimized_2020-1,
	title = {An optimized model based on convolutional neural networks and orthogonal learning particle swarm optimization algorithm for plant diseases diagnosis},
	volume = {52},
	issn = {2210-6502},
	url = {https://www.sciencedirect.com/science/article/pii/S2210650219305462},
	doi = {10.1016/j.swevo.2019.100616},
	abstract = {The plant disease classification based on using digital images is very challenging. In the last decade, machine learning techniques and plant images classification tools such as deep learning can be used for recognizing, detecting and diagnosing plant diseases. Currently, deep learning technology has been used for plant disease detection and classification. In this paper, an ensemble model of two pre-trained convolutional neural networks ({CNNs}) namely {VGG}16 and {VGG}19 have been developed for the task plant disease diagnosis by classifying the leaves images of healthy and unhealthy. In this context, {CNNs} are used due to its capability of overcoming the technical problems which are associated with the classification problem of plant diseases. However, {CNNs} suffer from a great variety of hyperparameters with specific architectures which is considered as a challenge to identify manually the optimal hyperparameters. Therefore, orthogonal learning particle swarm optimization ({OLPSO}) algorithm is utilized in this paper to optimize a number of these hyperparameters by finding optimal values for these hyperparameters rather than using traditional methods such as the manual trial and error method. In this paper, to prevent {CNNs} from falling into the local minimum and to train efficiently, an exponentially decaying learning rate ({EDLR}) schema is used. In this paper, the problem of the imbalanced used dataset has been solved by using random minority oversampling and random majority undersampling methods, and some restrictions in terms of both the number and diversity of samples have been overcome. The obtained results of this work show that the accuracy of the proposed model is very competitive. The experimental results are compared with the performance of other pre-trained {CNN} models namely {InceptionV}3 and Xception, whose hyperparameters were selected using a non-evolutionary method. The comparison results demonstrated that the proposed diagnostic approach has achieved higher performance than the other models.},
	pages = {100616},
	journaltitle = {Swarm and Evolutionary Computation},
	shortjournal = {Swarm and Evolutionary Computation},
	author = {Darwish, Ashraf and Ezzat, Dalia and Hassanien, Aboul Ella},
	urldate = {2021-12-02},
	date = {2020-02-01},
	langid = {english},
	keywords = {Convolutional neural networks ({CNNs}), Deep learning, Ensemble learning, Hyperparameters optimization, Imbalanced data, Orthogonal learning particle swarm optimization ({OLPSO}), Plant disease classification, Transfer learning},
}

@inproceedings{ali_hybrid_2018,
	title = {A hybrid Particle swarm optimization -Extreme Learning Machine approach for Intrusion Detection System},
	doi = {10.1109/SCORED.2018.8711287},
	abstract = {There are several limitations that facing intrusion-detection system in current days, such as high rates of false positive alerts, low detection rates of rare but dangerous attacks. Daily, there are reports of incidents such as major ex-filtration of data for the purposes of stealing identities. Hybrid model's approaches have been widely used to increase the effectiveness of intrusion-detection platforms. This work proposes the extreme learning machine ({ELM}) is one of the poplar machine learning algorithms which, easy to implement with excellent learning performance characteristics. However, the internal power parameters (weight and basis) of {ELM} are initialized at random, causing the algorithm to be unstable. The Particle swarm optimization ({PSO}) is a well-known meta-heuristic which is used in this research to optimize the {ELM}. Our propose model has been apple based as intrusion detection and validated based on {NSL}-{KDD} data set. Our developed model has been compared against a basic {ELM}. {PSO}-{ELM} has outperformed a basic model in the testing accuracy.},
	eventtitle = {2018 {IEEE} Student Conference on Research and Development ({SCOReD})},
	pages = {1--4},
	booktitle = {2018 {IEEE} Student Conference on Research and Development ({SCOReD})},
	author = {Ali, Mohammed Hasan and Fadlizolkipi, Mohamad and Firdaus, Ahmad and Khidzir, Nik Zulkarnaen},
	date = {2018-11},
	keywords = {Data models, Intrusion detection, Machine learning algorithms, {NSL}-{KDD}, Neurons, Optimization, Particle swarm optimization, Support vector machines, extreme learning machine, hybrid, intrusion detection system},
}

@article{hassan_1st_university_heart_2019,
	title = {Heart Disease Prediction and Classification Using Machine Learning Algorithms Optimized by Particle Swarm Optimization and Ant Colony Optimization},
	volume = {12},
	issn = {21853118},
	url = {http://www.inass.org/2019/2019022824.pdf},
	doi = {10.22266/ijies2019.0228.24},
	abstract = {The prediction of heart disease is one of the areas where machine learning can be implemented. Optimization algorithms have the advantage of dealing with complex non-linear problems with a good flexibility and adaptability. In this paper, we exploited the Fast Correlation-Based Feature Selection ({FCBF}) method to filter redundant features in order to improve the quality of heart disease classification. Then, we perform a classification based on different classification algorithms such as K-Nearest Neighbour, Support Vector Machine, Naïve Bayes, Random Forest and a Multilayer Perception {\textbar} Artificial Neural Network optimized by Particle Swarm Optimization ({PSO}) combined with Ant Colony Optimization ({ACO}) approaches. The proposed mixed approach is applied to heart disease dataset; the results demonstrate the efficacy and robustness of the proposed hybrid method in processing various types of data for heart disease classification. Therefore, this study examines the different machine learning algorithms and compares the results using different performance measures, i.e. accuracy, precision, recall, f1-score, etc. A maximum classification accuracy of 99.65\% using the optimized model proposed by {FCBF}, {PSO} and {ACO}. The results show that the performance of the proposed system is superior to that of the classification technique presented above.},
	pages = {242--252},
	number = {1},
	journaltitle = {International Journal of Intelligent Engineering and Systems},
	shortjournal = {{IJIES}},
	author = {{Hassan 1st University} and Khourdifi, Youness and Bahaj, Mohamed and {Hassan 1st University}},
	urldate = {2021-12-02},
	date = {2019-02-28},
	langid = {english},
}

@inreference{noauthor_swarm_2021,
	title = {Swarm behaviour},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Swarm_behaviour&oldid=1051337808},
	abstract = {Swarm behaviour, or swarming, is a collective behaviour exhibited by entities, particularly animals, of similar size which aggregate together, perhaps milling about the same spot or perhaps moving en masse or migrating in some direction. It is a highly interdisciplinary topic. As a term, swarming is applied particularly to insects, but can also be applied to any other entity or animal that exhibits swarm behaviour. The term flocking or murmuration can refer specifically to swarm behaviour in birds, herding to refer to swarm behaviour in tetrapods, and shoaling or schooling to refer to swarm behaviour in fish. Phytoplankton also gather in huge swarms called blooms, although these organisms are algae and are not self-propelled the way animals are. By extension, the term "swarm" is applied also to inanimate entities which exhibit parallel behaviours, as in a robot swarm, an earthquake swarm, or a swarm of stars.
From a more abstract point of view, swarm behaviour is the collective motion of a large number of self-propelled entities. From the perspective of the mathematical modeller, it is an emergent behaviour arising from simple rules that are followed by individuals and does not involve any central coordination. Swarm behaviour is also studied by active matter physicists as a phenomenon which is not in thermodynamic equilibrium, and as such requires the development of tools beyond those available from the statistical physics of systems in thermodynamic equilibrium. In this regard, swarming has been compared to the mathematics of superfluids, specifically in the context of starling flocks (murmuration).Swarm behaviour was first simulated on a computer in 1986 with the simulation program boids. This program simulates simple agents (boids) that are allowed to move according to a set of basic rules. The model was originally designed to mimic the flocking behaviour of birds, but it can be applied also to schooling fish and other swarming entities.},
	booktitle = {Wikipedia},
	urldate = {2021-12-02},
	date = {2021-10-22},
	langid = {english},
	note = {Page Version {ID}: 1051337808},
}

@article{eberhart_particle_nodate,
	title = {Particle Swarm Optimization},
	pages = {59},
	author = {Eberhart, Russell C},
	langid = {english},
}

@inreference{noauthor_list_2021,
	title = {List of professional sports leagues by revenue},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=List_of_professional_sports_leagues_by_revenue&oldid=1058117542},
	abstract = {In this table, the "Season" column refers to the league season for which financial data is available and referenced, which is usually not the most recently completed season of competition. Revenues are listed in millions of euros.},
	booktitle = {Wikipedia},
	urldate = {2021-12-02},
	date = {2021-12-01},
	langid = {english},
	note = {Page Version {ID}: 1058117542},
}

@online{august_2018_0_astroball_2018,
	title = {‘Astroball’ Shows Us How the Astros Won It All—and How They Might Do It Again},
	url = {https://www.texasmonthly.com/arts-entertainment/astroball-shows-us-how-the-astros-won-it-all-how-they-might-do-it-again/},
	abstract = {Ben Reiter’s new book 'Astroball' offers a comprehensive account of how the Astros became the next American baseball dynasty.},
	titleaddon = {Texas Monthly},
	author = {August 2018 0, Michael Hardy},
	urldate = {2021-12-02},
	date = {2018-07-27},
	langid = {english},
}

@inreference{noauthor_moneyball_2021,
	title = {\textit{Moneyball}},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Moneyball&oldid=1056362363},
	abstract = {Moneyball: The Art of Winning an Unfair Game is a book by Michael Lewis, published in 2003, about the Oakland Athletics baseball team and its general manager Billy Beane. Its focus is the team's analytical, evidence-based, sabermetric approach to assembling a competitive baseball team despite Oakland's small budget. A film based on Lewis' book, starring Brad Pitt and Jonah Hill, was released in 2011.},
	booktitle = {Wikipedia},
	urldate = {2021-12-01},
	date = {2021-11-21},
	langid = {english},
	note = {Page Version {ID}: 1056362363},
}

@inreference{noauthor_moneyball_2021-1,
	title = {\textit{Moneyball} (film)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Moneyball_(film)&oldid=1057665301},
	abstract = {Moneyball is a 2011 American biographical film directed by Bennett Miller and written by Steven Zaillian and Aaron Sorkin. The film is based on the 2003 nonfiction book by Michael Lewis, an account of the Oakland Athletics baseball team's 2002 season and their general manager Billy Beane's attempts to assemble a competitive team. In the film, Beane (Brad Pitt) and assistant general manager Peter Brand (Jonah Hill), faced with the franchise's limited budget for players, build a team of undervalued talent by taking a sophisticated sabermetric approach to scouting and analyzing players.
Columbia Pictures bought the rights to Lewis's book in 2004, hiring Stan Chervin to write the screenplay. David Frankel was initially set to direct with Zaillian now writing the screenplay, but was soon replaced by Steven Soderbergh, who planned to make the film in a semi-documentary style featuring interviews from real athletes, and having the real players and coaches on the team portray themselves. However, prior to its July 2009 filming start, the film was put in turnaround due to creative differences between Soderbergh and Sony over a last-minute script rewrite. Soderbergh exited, and Miller was hired to direct, with Pitt becoming a producer and Sorkin being hired to provide rewrites. Filming began in July 2010, taking place at various stadiums such as Dodger Stadium and Oakland Coliseum.
Moneyball premiered at the 2011 Toronto International Film Festival and was released on September 23, 2011, to box office success and critical acclaim, particularly for its acting and screenplay. The film was nominated for six Academy Awards, including Best Picture, Best Adapted Screenplay, Best Actor for Pitt and Best Supporting Actor for Hill.},
	booktitle = {Wikipedia},
	urldate = {2021-12-01},
	date = {2021-11-28},
	langid = {english},
	note = {Page Version {ID}: 1057665301},
}

@online{noauthor_google_nodate,
	title = {Google Scholar},
	url = {https://scholar.google.com/},
	urldate = {2021-12-01},
}

@online{noauthor_my_nodate,
	title = {My Library {\textbar} Zotero},
	url = {https://www.zotero.org/cweatherly/library},
	urldate = {2021-09-22},
}
