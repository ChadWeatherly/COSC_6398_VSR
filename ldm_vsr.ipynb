{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6be0b271d4abd02",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Diffusion Model for Video Super Resolution\n",
    "\n",
    "Inspiration gathered from:\n",
    "\n",
    "https://github.com/CompVis/latent-diffusion\n",
    "\n",
    "https://ar5iv.labs.arxiv.org/html/2311.15908"
   ]
  },
  {
   "cell_type": "code",
   "id": "6840a1735dd2c511",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T00:52:36.635886Z",
     "start_time": "2024-04-14T00:52:34.626385Z"
    }
   },
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # provides functions that don't need to be in a computational graph, i.e. aren't part of a NN, usually for single-use\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "set('PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512')\n",
    "\n",
    "\"\"\"\n",
    "if matplotlib doesn't run, go into the envs/pytorch_vsr environment in anaconda3\n",
    "and delete all version of libiomp5md.dll , and it should work\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device='cpu'\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1de92679892104e7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T00:52:36.639869Z",
     "start_time": "2024-04-14T00:52:36.636890Z"
    }
   },
   "source": [
    "torchvision.__version__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.2+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3783a106bfa2d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T00:52:36.647205Z",
     "start_time": "2024-04-14T00:52:36.639869Z"
    }
   },
   "source": [
    "# # datasets with _sharp are the correct/ground truth images\n",
    "# # datasets with _blur_bicubic are those that have been blurred and\n",
    "# # downsampled using bicubic interpolation\n",
    "# datasets = ['train_sharp', 'train_blur_bicubic', 'val_sharp', 'val_blur_bicubic']\n",
    "# for set in datasets:\n",
    "#     print(set)\n",
    "#     if not os.path.isfile(f\"REDS/{set}.zip\"):\n",
    "#         # print(\"Downloading\")\n",
    "#         cmdlet = f\"python download_REDS.py --{set}\"\n",
    "#         print(cmdlet)\n",
    "#         os.system(cmdlet)\n",
    "# # if not already downloaded, this will download all datasets (takes a while)\n",
    "# \n",
    "# # Set up data into dataset and dataloader\n",
    "# # It assumes the project file structure as downloaded from above\n",
    "# # Built based on docs: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# class REDS(Dataset):\n",
    "#     def __init__(self, train=True, device='cuda'):\n",
    "#         self.device = device\n",
    "#         self.type = 'train' if train else 'test' \n",
    "#         if self.type == 'train':\n",
    "#             self.hr_dir = \"REDS/train_sharp/train/train_sharp\"\n",
    "#             self.lr_dir = \"REDS/train_blur_bicubic/train/train_blur_bicubic/X4\"\n",
    "#         else:\n",
    "#             self.hr_dir = \"REDS/val_sharp/val/val_sharp\"\n",
    "#             self.lr_dir = \"REDS/val_blur_bicubic/val/val_blur_bicubic/X4\"\n",
    "#             \n",
    "#     def __len__(self):\n",
    "#         return len(os.listdir(self.hr_dir)) # training size = 240 videos, testing size = 30 videos\n",
    "#             \n",
    "#     def __getitem__(self, idx):\n",
    "#         # each return gives a single HR frame with 5 corresponding LR frames\n",
    "#         # the middle LR frame (frame 3) will be the blurred/downsampled version of the HR frame\n",
    "#         # the 5 sequential LR frames will be chosen randomly from the given idx-video\n",
    "#         \n",
    "#         # Getting video sequence folder name\n",
    "#         if idx < 10:\n",
    "#             video = '00' + str(idx)\n",
    "#         elif idx < 100:\n",
    "#             video = '0' + str(idx)\n",
    "#         else:\n",
    "#             video = str(idx)\n",
    "#         # Getting random sequence of 5 LR frames from the video    \n",
    "#         num_video_frames = len(os.listdir(f\"{self.hr_dir}/000\"))\n",
    "#         rand_frame_id = np.random.randint(2, num_video_frames - 2)\n",
    "#         lr_frame_idx = []\n",
    "#         for i in range(-2, 3):\n",
    "#             id_int = rand_frame_id + i\n",
    "#             if id_int < 10:\n",
    "#                 id_str = '0000000' + str(id_int)\n",
    "#             elif id_int < 100:\n",
    "#                 id_str = '000000' + str(id_int)\n",
    "#             else:\n",
    "#                 id_str = '00000' + str(id_int)\n",
    "#             lr_frame_idx.append(id_str)\n",
    "#         # Actually reading in the images\n",
    "#         hr_frame = torchvision.io.read_image(f\"{self.hr_dir}/{video}/{lr_frame_idx[2]}.png\").to(self.device)\n",
    "#         lr_frames = []\n",
    "#         for v in lr_frame_idx:\n",
    "#             lr_frame = torchvision.io.read_image(f\"{self.lr_dir}/{video}/{v}.png\").to(self.device)\n",
    "#             lr_frames.append(lr_frame)\n",
    "#         lr_frames = torch.stack(lr_frames).permute(1, 0, 2, 3)\n",
    "#         # hr_frame is of size 3x720x1280 (CxHxW)\n",
    "#         # lr_imgs of of size 5x3x180x320 (TxCxHxW)\n",
    "#         # where C=channel, T=time (video sequence)\n",
    "#         return torch.tensor(lr_frames).float(), hr_frame.float()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8859036c19a75c4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T00:52:36.972890Z",
     "start_time": "2024-04-14T00:52:36.647205Z"
    }
   },
   "source": [
    "# If using REDS\n",
    "# train_dataset = REDS(train=True, device=device)\n",
    "# test_dataset = REDS(train=False, device=device)\n",
    "\n",
    "# If using moving MNIST : http://www.cs.toronto.edu/~nitish/unsupervised_video/\n",
    "# This will download all the data\n",
    "data = torchvision.datasets.MovingMNIST(root='./', split=None,\n",
    "                                        split_ratio=10, download=True)\n",
    "train_data = data[0:8000].to('cpu')      # 80% for train\n",
    "test_data = data[8000:10001].to('cpu')    # 20% for test\n",
    "\n",
    "def transform_image(image):\n",
    "    # Performs downsampling and then blurring\n",
    "    image = torchvision.transforms.functional.resize(image, (32, 32))\n",
    "    gb = torchvision.transforms.GaussianBlur(kernel_size=(3,3))\n",
    "    image = gb(image)\n",
    "    return image\n",
    "    \n",
    "def get_sample(data, idx):\n",
    "    \"\"\"\n",
    "    Given a dataset (train/test) and a sample idx, a random sequence\n",
    "    of 5 LR frames is computed\n",
    "    \"\"\"\n",
    "    num_video_frames = data[idx].size()[0]\n",
    "    rand_frame_idx = np.random.randint(2, num_video_frames - 2)\n",
    "    # Get HR frame\n",
    "    hr_frame = data[idx][rand_frame_idx].to(device)\n",
    "    # Get LR frames\n",
    "    lr_frames = [data[idx][k] for k in range(rand_frame_idx-2, rand_frame_idx+3)]\n",
    "    lr_frames = [transform_image(frame) for frame in lr_frames]\n",
    "    lr_frames = torch.stack(lr_frames, dim=0).to(device).permute(1, 0, 2, 3) # permuted to have channel first (for 3D Conv)\n",
    "    return lr_frames, hr_frame\n",
    "\n",
    "lr_frames, hr_frame = get_sample(data, 0)\n",
    "lr_frames = lr_frames.permute(1, 0, 2, 3)\n",
    "print('Comparing the 5 low-res images to the one high-res one')\n",
    "for f in lr_frames:\n",
    "    pil_fr = torchvision.transforms.ToPILImage()(f)\n",
    "    display(pil_fr)\n",
    "display(torchvision.transforms.ToPILImage()(hr_frame))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the 5 low-res images to the one high-res one\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABCklEQVR4AWNgGBGAEYsvGQXYPrEzffoDlsKmgLvV6Lron/rrYAUsMBMYeTiZODh/PfjLwMAqJC3Ewq+LqoDdIUru4x0l1uRXDAyfGjk/yE5khWiFmWDZ8PP0gceNL74Bhf/d4+aW5v3E+B9mOohWcJJhY007aQV2k/S0g3e+tHOB5WEmPHjAyG0fMv0MWBeH2NN/HDd+gxVAfcGtIKWvIzF/4w+ILoF/xTrJb5BM4EzTFrZkPrgTIs/w542Qwd3PYHkGJhDFGpAkI7moVVYUIgYk+SW+/INwwG5g03p3a8sxqUjF2zAV/w+ugwQkVIBPkoOBgbc7BCbPwMQDNhrOhzBEBdEERrmDKwQAWBdG1aRvIq4AAAAASUVORK5CYII=",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+ilCs3QE/QUhBBwetT2dtLeXcUEMMkzu2BHEMs3rivQNZ0vStE0Sa41Hwt/ZrTwGPT457h5J5nOP3jYIChRz05JFZnw31Way1t7OPV101LsohJsxcGVtw2qARx1PNZvjyS3k8ea29o4eE3b4ITaOvPH1zWbo+t6hoF6bzTZxDcFGj37FbAPXGQal1DxFqOq6dHaX8xuNlw84mkJL7mABGfT5RVGzu57C9hvLWQxzwuJI3A+6wOQajmlknmeaVy8kjFmY9STyTTKKKK//Z"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABPklEQVR4Ae1Ry0rDQBSduTNtHr6IKUksFREVQS0K/v8PuHJXFauN0mpS0tamad4Zm2QIhODOjeBsZi7n3HPOvYPQHzi4kREDIIZYxkqENggtXROzeG6tfiJIp7d6Fgzu1lnBaCpgKsqClLy8NQhAAIAl0Ygp3TMi8HSVAtnWVCGl8XRiE2ZQqU3jmgXeO79UfXeXPTnQu+nrrxTKkPxCWFYV7FNNE9nGiCSeV0ZAlUVsRev4yF2Ya/ocEWP0EdXHxMHnaktTZo9mGJrxiTCdpzVCa/+ga/Rk+/7hi6EMUncelji3IJ3+1eFOxx2PnbwxMK137sAJ7d71BbaXKEF5NOYOwE1qChgth/aQHqcy5Aw/QIz/FVdIJilxHCkQ/aKvQjcVXygBlGaC2vJmfHxuUBGKGgNmfLgK/3/82ga+AdUrfJXdrgV9AAAAAElFTkSuQmCC",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+itC20TU7y2NzbWcssI6uq5FUCCrEEYI4IoAJIAGSa7jS/Dujyaek93DdqApaWZxtQewrS8AapZwzX1s1xc+TGrtHGD8u33rzu8dZL2d0+60jEfnS2N19ivYrny1kMbbtrdDXSal47vtW065srpEEMmNiIMBK5eG4ltyxikZCwwcHqKjoooor/9k="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABMElEQVR4Ae2SS0+DQBCAZ5dXF1rB1sam2nAgjXowXjz4/8/eqokamyhibS2EQqE8RNakwlLRnjx4cS87mfny7cxkAf7+oHoLGCOgkOdlni+D4kZqT0N5+OpkRaIO4L3zI5G+XK6WWwDgBELU1vxuG0Cdka2eKA1mZkFhpP4EiUiSBEQ/M5sAT2QRN/ShYeQS9wNA+sY+rJqDtgJxkBRGqAxYG542XdJXZ8EyNb1yERUAOHdnrgr2CHbSefTdAOF48t5SvafxYez7paB6AhFZ7nS7onU9FW8CtxQwACm6ofcOsqv7x9RyUr8YEhggDC7OdjMSJH5E3QVldQbwWoezHrg2KIhulCsD9W6fp6Z4LCDM+lv3UY6ZmDaEsYykRWVfA18/DEe4t6hGsHn+g99t4AOmomnsBngeDAAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+it5/B+rx6T/aZgU2wXcWDAkD6Vg0V0fh7RLHUoWa6e480nCJEmfxJrs573R9J8LarY2u6WRIxGXeTOSewFeU0Dgiu2sfiC9jaxWcFhDDbiMpIVX5n981x00xkmkZS212JwTUVFFFFf/Z"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABMElEQVR4Ae2STU+DQBCGd2HDl2BtDaWgMbEmvahR//9f8ODFpGk8gKhEEotCKfLRXbfV7i5EDx68OafZ2Wdn3plZAP7cYLsClCQawJiwMGLexlGcoY6r5Dlj4Q6gnVyOSDG9zvGW6AAQqbqm1f79T0AZkP2DMVK5tE6GMpahi3RV3lYAHYAo3tmF3cjrXj6Ne+szlWAYBim4xlYGqA4cz27SNIyqr/egBSiHp8coXRT4JVkxQCyBdvt6VVlmM39n960MJA8eM9N8CsL6WwCSV7zTc8D0VgT4RKDpeK5r13c3fs53JZTQx+eTQc+cJaF4zwFpb3I1fHuwMiDsmkrhk5TKKPLjI0tR+a5FgCxmcRHnyahgi9y0IoiUIcagb63mS94knb54oD79chT7t99P4AMk4mZLW1mjvAAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+pIIvPnji3Bd7AZPQV0GteD7nRtMS/N1BPCzBcxnODXN0VraDp1tf3ZF1c+RGnPC5Le1dr431G2g8IWGmQLEHZtzADDAeprzSiuu8P+L7XQbGOOLTIpLnfl5pBnisPXtQXVNXmu03bZDkA9qzaKKKK//Z"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABTUlEQVR4AeWSv0vDQBTH7y6XH7WNMaC2odSq7RCKIoqDOjjo4n/rqINjB0c7VIKEIrXUiia1EU1M0sQIJpcHZnL0wXHHvQ/f9733DqG/BgYCmOAYoShZWQAAlzQ5wMGTE2V5RNkRIbxy1gk467LnslsI0JLCVRp3RhEQPV/1eH2HJ0wAlojfh0FNFHnu2+pPgBKIrJ5stcqqmGaTPa+WuKQ0DBzrMwdAhXB00TwVjDdWAXpAkeNVyMwhdJ4hUAFLVb0x2ZQfx35aBQC4rB/uyva+1J0WAEpn/eXGbskR6zVQiF3jwfbavGmyVgIAu/eSVG/PrkdhagG8gij16tqGOu72P7J8HsDK3oFWU73bQcG4+ebxtj8xlgWSNSERynnghNAc9N0jaYHMfy3hD87j6auwqDGHCZf/coTEUUyXZCs/C6aVngiFE07v//n+BePzZ6NbfKm0AAAAAElFTkSuQmCC",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+r2kWltfanFb3d19mhc4aXbux+Fanirw1F4faze3vPtUNzHvV9hXv6GudorQ0SOeTV7cW0kccwbKvKQFX3Oa7r4iJLqenWl//AGnBcxWkawHYMbnPXHrXmtFdD4ZvtB04Tz6tZyXcwx5MYOF/GpfF+t22ry2jWCiC2EQzbIMLG3euZoooor//2Q=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAABxklEQVR4Ae1Tv0sCURz/KomE0NK5BQ5C3uEQdLQEalNzCv0JSatEJNok9jcIuYhDky4NOZpTg7VlDREZRPaoJRJa7nt276mnXif37oKmew+O76/P98fnvg/APS4DLgOcDIQHpLDv32sWmsucCENYGccnavBQ1Wtim2s6NPEsmNgMps9v3zs1CT6Dg1tNpFjoDVZMIBwdQIvhAn4TuB0OtiRgkxjScJO4FAY4N4CpyjPC4tpuBIQNgOzro0kKa9PqeA/w9HewVQch+BB1VM9WgvhO7N4jPoMU8VzASwXK0V7yWs9lKYTqKtLbzWXIAGn4FZ5ZoiYBElFQu6VtAWQFC5pjs4/HE7+l1Ka1hwtYxz5l4QDR7C3Ny5SktdlJoZKjwqW9BEOw9pWJSpjiNEFLIes0wcqDow4CdZVIrIEE4pGXSTY+8XRnVF97dCdZjw1oSJaLd3QPajZAU6F5ugUdugls/ikPn5jBdkwuqtj/Ukt8CEMUUWLa38dbsYpvBhePqv35jjY9yQOkVQzyQGZjWsNXQKcXUEnPOnm0YKNbTY/2WEUHCbTCep2a0tBlR0Lw6Y8JQJx046gDF+Qy4DLgMuAy8L8M/ACTLq6KKxW3RwAAAABJRU5ErkJggg==",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiivTfB/wbvfGXg9dctNZtLeV5njS3njO3C8ZLjOCT22njnPaqHxB+Ft54B0+wvJtRgvY7qRoyYkKhGCg9zzn5v++ffFcDRRRRWlbeHtavYUmtdIv54n+7JHbOyt64IGK7Lwt8KNZ120uZNWvB4fsYcOrahGyCU85ZQcA4xyc8ZHrXX/ABT8N2Xhz4Q+GtNtdYtLhLadpFK/eui+SXTBIwNxz25H0PhlFFFaGiahDpOtWl/cWEF/FBJva1nzskx0Bx2zz+HNelQ/EH4kfEPUotJ0aQ2NvIwUrYReWkK8DLScsAB7j6dKz/iZ4ms7m10nwZot815puk/668kckXNychnyTyoy2D/tHHGDU3xsurAazoej6ZdRTW+l6ZHAUiYMsb5ORkcE7QteX0UUV7J4Is/Amh+DE1TVfEOmnU7tw8qtZi5lgjXrCkbggOT1cqR6Ajmuns/iFB8SLDxN4TsbN7SzTR5ZLN2bEsroR94LgAEkfKo6A9uB86UUUUUUVYsr67066W6srmW3uFBCyxOVYAgg4I9QSKr0UUUUUUUUUUUUUUUUUUV//9k="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "566a961439deb8ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T00:52:37.672397Z",
     "start_time": "2024-04-14T00:52:36.972890Z"
    }
   },
   "source": [
    "# Set up loss functions\n",
    "\n",
    "# perceptual_loss\n",
    "vgg = torchvision.models.vgg19(weights='VGG19_Weights.IMAGENET1K_V1').features # removes final classification layer as we don't need it\n",
    "vgg.to(device)\n",
    "vgg.eval() # sets the model to evaluation mode, to not update weights/parameters\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False # don't calculate gradients after forward passes, reduces computation\n",
    "    \n",
    "# A hook function can be used on any nn.module(), like any layer of a neural net\n",
    "# Depending on whether it's a backwards hook (backprop) or forward hook (forward pass),\n",
    "# the inputs and outputs of that nn.module can be accessed for that operation\n",
    "# It's like an event, that each time it happens, the inputs/outputs of that module are saved and then used by vgg_hook\n",
    "vgg_activations = {3:torch.tensor(0),\n",
    "                   8:torch.tensor(0),\n",
    "                   17:torch.tensor(0),\n",
    "                   26:torch.tensor(0),\n",
    "                   35:torch.tensor(0)}\n",
    "def vgg_hook(module, input, output):   \n",
    "    # Get activations at several layers\n",
    "    for layer in [3, 8, 17, 26, 35]: # each of the layers we want\n",
    "        if module == vgg[layer]: \n",
    "            vgg_activations[layer] = output\n",
    "    \n",
    "for layer in [3, 8, 17, 26, 35]:\n",
    "    vgg[layer].register_forward_hook(vgg_hook)\n",
    "\n",
    "def perceptual_loss(generated_img, target_img): # we want a lower value\n",
    "    _ = vgg(generated_img)\n",
    "    generated_activations = vgg_activations.copy()\n",
    "    _ = vgg(target_img)\n",
    "    target_activations = vgg_activations.copy()\n",
    "    \n",
    "    # calculate F1 (mean absolute loss (MAE)) for each activation layer\n",
    "    mae_loss = []\n",
    "    for layer in [3, 8, 17, 26, 35]:\n",
    "        mae = F.l1_loss(generated_activations[layer],\n",
    "                        target_activations[layer])\n",
    "        mae_loss.append(mae)\n",
    "    \n",
    "    return sum(mae_loss)\n",
    "\n",
    "# Test if perceptual loss works\n",
    "lr_imgs1, hr_img1 = get_sample(train_data, 20)\n",
    "lr_imgs2, hr_img2 = get_sample(train_data, 30)\n",
    "\n",
    "print(f\"Differing video sequences: {perceptual_loss(hr_img1.expand(3,-1,-1).float(), \n",
    "                                                    hr_img2.expand(3,-1,-1).float())}\")\n",
    "print(f\"Same frame: {perceptual_loss(hr_img2.expand(3,-1,-1).float(), \n",
    "                                                    hr_img2.expand(3,-1,-1).float())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differing video sequences: 171.27947998046875\n",
      "Same frame: 0.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T00:52:37.675756Z",
     "start_time": "2024-04-14T00:52:37.673401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters for training/testing\n",
    "new_model = True\n",
    "num_epochs = 5\n",
    "learning_rate = 0.00001\n",
    "batch_size = 100\n",
    "train=True # if false, then the models will initialize with best weights (for inference/testing)"
   ],
   "id": "4799eeebab0107b2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T00:52:37.680949Z",
     "start_time": "2024-04-14T00:52:37.676761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# diff_model = diffusion_vsr(s=torch.tensor(0.0008), \n",
    "#                            train_T=torch.tensor(500), \n",
    "#                            infer_T=torch.tensor(50)).to(device)\n",
    "# diff_model.calc_train_steps()\n",
    "# \n",
    "# lr_imgs, hr_img = get_sample(train_data, 20)\n",
    "# t = torch.randint(low=0, high=50, size=(1,)).to(device)\n",
    "# t = 25\n",
    "# diff_model(hr_img, lr_imgs, t)\n",
    "# \n",
    "# noisy_img = diff_model.add_noise(hr_img, 4)\n",
    "# display(torchvision.transforms.ToPILImage()(hr_img))\n",
    "# display(torchvision.transforms.ToPILImage()(noisy_img))"
   ],
   "id": "2063775ec941207c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-14T00:52:37.680949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# Can be either 'mse' or 'pq'\n",
    "loss_function = 'pq'\n",
    "path = f'saved_models/diff_model_{loss_function}.pt'\n",
    "# Initialize Model architecture\n",
    "pq_diff_model = diffusion_vsr(s=torch.tensor(0.0008), \n",
    "                           train_T=torch.tensor(1000), \n",
    "                           infer_T=torch.tensor(50)).to(device)\n",
    "if new_model:\n",
    "    # Train/Test \n",
    "    # Mean Square Error/Perceptual Quality\n",
    "    train_mse_loss = []\n",
    "    train_pq_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_pq_loss = []\n",
    "    best_val_loss = float('inf')\n",
    "else:\n",
    "    # Get dictionary of info\n",
    "    old_model = torch.load(path, map_location=device)\n",
    "    # load model with previous weights/parameters\n",
    "    pq_diff_model.load_state_dict(old_model[0])\n",
    "    # update previous lists of loss\n",
    "    if loss_function == 'mse':\n",
    "        train_mse_loss = old_model[1]\n",
    "        test_mse_loss = old_model[2]\n",
    "    else:\n",
    "        train_pq_loss = old_model[1]\n",
    "        test_pq_loss = old_model[2]\n",
    "    best_val_loss = old_model[3]\n",
    "    \n",
    "if train:\n",
    "    start_time = time.time()\n",
    "    mse = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params=pq_diff_model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        # Training\n",
    "        pq_diff_model.train()\n",
    "        pq_diff_model.calc_train_steps()\n",
    "        running_loss = 0\n",
    "        for i in range(len(train_data)):\n",
    "            lr_imgs, hr_img = get_sample(train_data, i)\n",
    "            # random t-step to predict\n",
    "            t = torch.randint(low=0, high=1000, size=(1,)).to(device)\n",
    "            # hr_img after t steps of noise addition\n",
    "            hr_t = pq_diff_model.add_noise(hr_img, t)\n",
    "            # hr_img after t-1 steps of noise addition\n",
    "            # This is what we want the model to predict (bc we want the reverse process)\n",
    "            hr_tmin1 = pq_diff_model.add_noise(hr_img, t-1)\n",
    "            # Now, let's do one pass through the model\n",
    "            pred_hr_tmin1 = pq_diff_model(hr_img, lr_imgs, t)\n",
    "            if loss_function == 'mse':\n",
    "                loss = mse(pred_hr_tmin1, \n",
    "                           hr_tmin1)\n",
    "            else:\n",
    "                loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "                                       hr_tmin1.expand(3,-1,-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            del lr_imgs, hr_img, t, hr_t, hr_tmin1, pred_hr_tmin1\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i%batch_size == 0 and i != 0:\n",
    "                avg_running_loss = running_loss / batch_size\n",
    "                running_loss = 0\n",
    "                if loss_function == 'mse':\n",
    "                    train_mse_loss.append(avg_running_loss)\n",
    "                else:\n",
    "                    train_pq_loss.append(avg_running_loss)\n",
    "                print(f'Batch Avg. Training Loss: {avg_running_loss:.4f}')\n",
    "            \n",
    "        # Testing\n",
    "        pq_diff_model.eval()\n",
    "        pq_diff_model.calc_test_steps()\n",
    "        running_loss = 0\n",
    "        for i in range(len(test_data)):\n",
    "            with torch.no_grad():\n",
    "                lr_imgs, hr_img = get_sample(test_data, i)\n",
    "                # random t-step to predict\n",
    "                t = torch.randint(low=0, high=50, size=(1,)).to(device)\n",
    "                # hr_img after t steps of noise addition\n",
    "                hr_t = pq_diff_model.add_noise(hr_img, t)\n",
    "                # hr_img after t-1 steps of noise addition\n",
    "                # This is what we want the model to predict (bc we want the reverse process)\n",
    "                hr_tmin1 = pq_diff_model.add_noise(hr_img, t-1)\n",
    "                # Now, let's do one pass through the model\n",
    "                pred_hr_tmin1 = pq_diff_model(hr_t, lr_imgs, t)\n",
    "                if loss_function == 'mse':\n",
    "                    loss = mse(pred_hr_tmin1, hr_tmin1)\n",
    "                else:\n",
    "                    loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "                                           hr_tmin1.expand(3,-1,-1))\n",
    "                running_loss += loss.item()\n",
    "                del lr_imgs, hr_img, t, hr_t, hr_tmin1, pred_hr_tmin1\n",
    "            \n",
    "            if i%batch_size == 0 and i != 0:\n",
    "                avg_running_loss = running_loss / batch_size\n",
    "                running_loss = 0\n",
    "                if loss_function == 'mse':\n",
    "                    test_mse_loss.append(avg_running_loss)\n",
    "                else:\n",
    "                    test_pq_loss.append(avg_running_loss)\n",
    "                print(f'Batch Avg. Testing Loss: {avg_running_loss:.4f}')\n",
    "        \n",
    "                # Capture results\n",
    "                if avg_running_loss < best_val_loss:\n",
    "                    # print(avg_running_loss, best_val_loss)\n",
    "                    best_val_loss = avg_running_loss\n",
    "                    save_dict = dict()\n",
    "                    save_dict[0] = pq_diff_model.state_dict()\n",
    "                    if loss_function == 'mse':\n",
    "                        save_dict[1] = train_mse_loss\n",
    "                        save_dict[2] = test_mse_loss\n",
    "                    else:\n",
    "                        save_dict[1] = train_pq_loss\n",
    "                        save_dict[2] = test_pq_loss\n",
    "                    save_dict[3] = best_val_loss\n",
    "                    torch.save(save_dict, path)\n",
    "            \n",
    "        # Calculate epoch training/testing time\n",
    "        if epoch==0:\n",
    "            prev_time = start_time\n",
    "        else:\n",
    "            prev_time = curr_time    \n",
    "        curr_time = time.time()\n",
    "        time_diff = (curr_time - prev_time) # in seconds\n",
    "        minutes, seconds = divmod(time_diff, 60)\n",
    "        minutes = int(minutes)\n",
    "        seconds = int(seconds)\n",
    "        \n",
    "        print(f'Time:')\n",
    "        print(f'{minutes} minutes:{seconds} seconds')\n",
    "        print()"
   ],
   "id": "8bcebbe8401c70a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Batch Avg. Training Loss: 63.1309\n",
      "Batch Avg. Training Loss: 63.4700\n",
      "Batch Avg. Training Loss: 64.5027\n",
      "Batch Avg. Training Loss: 68.4942\n",
      "Batch Avg. Training Loss: 68.1776\n",
      "Batch Avg. Training Loss: 64.5914\n",
      "Batch Avg. Training Loss: 64.3996\n",
      "Batch Avg. Training Loss: 69.4531\n",
      "Batch Avg. Training Loss: 66.2932\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting avg. training and testing loss for MSE-trained model\n",
    "plt.plot(train_pq_loss)\n",
    "plt.plot(test_pq_loss)\n",
    "plt.title(\"Pereceptual Loss\")\n",
    "plt.legend(['Train', 'Test'])"
   ],
   "id": "c647af77cc73aacd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Can be either 'mse' or 'pq'\n",
    "loss_function = 'mse'\n",
    "path = f'saved_models/diff_model_{loss_function}.pt'\n",
    "# Initialize Model architecture\n",
    "mse_diff_model = diffusion_vsr(s=torch.tensor(0.0008), \n",
    "                           train_T=torch.tensor(1000), \n",
    "                           infer_T=torch.tensor(50)).to(device)\n",
    "if new_model:\n",
    "    # Train/Test \n",
    "    # Mean Square Error/Perceptual Quality\n",
    "    train_mse_loss = []\n",
    "    train_mse_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_mse_loss = []\n",
    "    best_val_loss = float('inf')\n",
    "else:\n",
    "    # Get dictionary of info\n",
    "    old_model = torch.load(path, map_location=device)\n",
    "    # load model with previous weights/parameters\n",
    "    mse_diff_model.load_state_dict(old_model[0])\n",
    "    # update previous lists of loss\n",
    "    if loss_function == 'mse':\n",
    "        train_mse_loss = old_model[1]\n",
    "        test_mse_loss = old_model[2]\n",
    "    else:\n",
    "        train_pq_loss = old_model[1]\n",
    "        test_pq_loss = old_model[2]\n",
    "    best_val_loss = old_model[3]\n",
    "    \n",
    "if train:\n",
    "    start_time = time.time()\n",
    "    mse = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params=mse_diff_model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        # Training\n",
    "        mse_diff_model.train()\n",
    "        mse_diff_model.calc_train_steps()\n",
    "        running_loss = 0\n",
    "        for i in range(len(train_data)):\n",
    "            lr_imgs, hr_img = get_sample(train_data, i)\n",
    "            # random t-step to predict\n",
    "            t = torch.randint(low=0, high=1000, size=(1,)).to(device)\n",
    "            # hr_img after t steps of noise addition\n",
    "            hr_t = mse_diff_model.add_noise(hr_img, t)\n",
    "            # hr_img after t-1 steps of noise addition\n",
    "            # This is what we want the model to predict (bc we want the reverse process)\n",
    "            hr_tmin1 = mse_diff_model.add_noise(hr_img, t-1)\n",
    "            # Now, let's do one pass through the model\n",
    "            pred_hr_tmin1 = mse_diff_model(hr_img, lr_imgs, t)\n",
    "            if loss_function == 'mse':\n",
    "                loss = mse(pred_hr_tmin1, \n",
    "                           hr_tmin1)\n",
    "            else:\n",
    "                loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "                                       hr_tmin1.expand(3,-1,-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i%batch_size == 0 and i != 0:\n",
    "                avg_running_loss = running_loss / batch_size\n",
    "                running_loss = 0\n",
    "                if loss_function == 'mse':\n",
    "                    train_mse_loss.append(avg_running_loss)\n",
    "                else:\n",
    "                    train_pq_loss.append(avg_running_loss)\n",
    "                print(f'Batch Avg. Training Loss: {avg_running_loss:.4f}')\n",
    "            \n",
    "        # Testing\n",
    "        mse_diff_model.eval()\n",
    "        mse_diff_model.calc_test_steps()\n",
    "        running_loss = 0\n",
    "        for i in range(len(test_data)):\n",
    "            with torch.no_grad():\n",
    "                lr_imgs, hr_img = get_sample(test_data, i)\n",
    "                # random t-step to predict\n",
    "                t = torch.randint(low=0, high=50, size=(1,)).to(device)\n",
    "                # hr_img after t steps of noise addition\n",
    "                hr_t = mse_diff_model.add_noise(hr_img, t)\n",
    "                # hr_img after t-1 steps of noise addition\n",
    "                # This is what we want the model to predict (bc we want the reverse process)\n",
    "                hr_tmin1 = mse_diff_model.add_noise(hr_img, t-1)\n",
    "                # Now, let's do one pass through the model\n",
    "                pred_hr_tmin1 = mse_diff_model(hr_t, lr_imgs, t)\n",
    "                if loss_function == 'mse':\n",
    "                    loss = mse(pred_hr_tmin1, hr_tmin1)\n",
    "                else:\n",
    "                    loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "                                           hr_tmin1.expand(3,-1,-1))\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if i%batch_size == 0 and i != 0:\n",
    "                avg_running_loss = running_loss / batch_size\n",
    "                running_loss = 0\n",
    "                if loss_function == 'mse':\n",
    "                    test_mse_loss.append(avg_running_loss)\n",
    "                else:\n",
    "                    test_pq_loss.append(avg_running_loss)\n",
    "                print(f'Batch Avg. Testing Loss: {avg_running_loss:.4f}')\n",
    "        \n",
    "                # Capture results\n",
    "                if avg_running_loss < best_val_loss:\n",
    "                    # print(avg_running_loss, best_val_loss)\n",
    "                    best_val_loss = avg_running_loss\n",
    "                    save_dict = dict()\n",
    "                    save_dict[0] = mse_diff_model.state_dict()\n",
    "                    if loss_function == 'mse':\n",
    "                        save_dict[1] = train_mse_loss\n",
    "                        save_dict[2] = test_mse_loss\n",
    "                    else:\n",
    "                        save_dict[1] = train_mse_loss\n",
    "                        save_dict[2] = test_mse_loss\n",
    "                    save_dict[3] = best_val_loss\n",
    "                    torch.save(save_dict, path)\n",
    "            \n",
    "        # Calculate epoch training/testing time\n",
    "        if epoch==0:\n",
    "            prev_time = start_time\n",
    "        else:\n",
    "            prev_time = curr_time    \n",
    "        curr_time = time.time()\n",
    "        time_diff = (curr_time - prev_time) # in seconds\n",
    "        minutes, seconds = divmod(time_diff, 60)\n",
    "        minutes = int(minutes)\n",
    "        seconds = int(seconds)\n",
    "        \n",
    "        print(f'Time:')\n",
    "        print(f'{minutes} minutes:{seconds} seconds')\n",
    "        print()"
   ],
   "id": "7db34393da7e6e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting avg. training and testing loss for MSE-trained model\n",
    "plt.plot(train_mse_loss)\n",
    "plt.plot(test_mse_loss)\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend(['Train', 'Test'])"
   ],
   "id": "9af5cab6cf256f62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# torchvision.transforms.ToPILImage()(hr_img)   ",
   "id": "beaea9b7c5fd0de6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# torchvision.transforms.ToPILImage()(hr_pred)",
   "id": "8d7be2c54e9aa500",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "29ef8b53f8d4e777",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
