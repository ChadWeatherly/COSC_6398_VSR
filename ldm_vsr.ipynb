{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6be0b271d4abd02",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Diffusion Model for Video Super Resolution\n",
    "\n",
    "Inspiration gathered from:\n",
    "\n",
    "https://github.com/CompVis/latent-diffusion\n",
    "\n",
    "https://ar5iv.labs.arxiv.org/html/2311.15908"
   ]
  },
  {
   "cell_type": "code",
   "id": "6840a1735dd2c511",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:38.989112Z",
     "start_time": "2024-04-16T22:39:38.238350Z"
    }
   },
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # provides functions that don't need to be in a computational graph, i.e. aren't part of a NN, usually for single-use\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "set('PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512')\n",
    "set('PYTORCH_ENABLE_MPS_FALLBACK=1')\n",
    "\n",
    "\"\"\"\n",
    "if matplotlib doesn't run, go into the envs/pytorch_vsr environment in anaconda3\n",
    "and delete all version of libiomp5md.dll , and it should work\n",
    "\"\"\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # Apple GPU\n",
    "else:\n",
    "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1de92679892104e7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:38.992996Z",
     "start_time": "2024-04-16T22:39:38.990076Z"
    }
   },
   "source": [
    "torchvision.__version__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3783a106bfa2d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:38.995787Z",
     "start_time": "2024-04-16T22:39:38.993590Z"
    }
   },
   "source": [
    "# # datasets with _sharp are the correct/ground truth images\n",
    "# # datasets with _blur_bicubic are those that have been blurred and\n",
    "# # downsampled using bicubic interpolation\n",
    "# datasets = ['train_sharp', 'train_blur_bicubic', 'val_sharp', 'val_blur_bicubic']\n",
    "# for set in datasets:\n",
    "#     print(set)\n",
    "#     if not os.path.isfile(f\"REDS/{set}.zip\"):\n",
    "#         # print(\"Downloading\")\n",
    "#         cmdlet = f\"python download_REDS.py --{set}\"\n",
    "#         print(cmdlet)\n",
    "#         os.system(cmdlet)\n",
    "# # if not already downloaded, this will download all datasets (takes a while)\n",
    "# \n",
    "# # Set up data into dataset and dataloader\n",
    "# # It assumes the project file structure as downloaded from above\n",
    "# # Built based on docs: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# class REDS(Dataset):\n",
    "#     def __init__(self, train=True, device='cuda'):\n",
    "#         self.device = device\n",
    "#         self.type = 'train' if train else 'test' \n",
    "#         if self.type == 'train':\n",
    "#             self.hr_dir = \"REDS/train_sharp/train/train_sharp\"\n",
    "#             self.lr_dir = \"REDS/train_blur_bicubic/train/train_blur_bicubic/X4\"\n",
    "#         else:\n",
    "#             self.hr_dir = \"REDS/val_sharp/val/val_sharp\"\n",
    "#             self.lr_dir = \"REDS/val_blur_bicubic/val/val_blur_bicubic/X4\"\n",
    "#             \n",
    "#     def __len__(self):\n",
    "#         return len(os.listdir(self.hr_dir)) # training size = 240 videos, testing size = 30 videos\n",
    "#             \n",
    "#     def __getitem__(self, idx):\n",
    "#         # each return gives a single HR frame with 5 corresponding LR frames\n",
    "#         # the middle LR frame (frame 3) will be the blurred/downsampled version of the HR frame\n",
    "#         # the 5 sequential LR frames will be chosen randomly from the given idx-video\n",
    "#         \n",
    "#         # Getting video sequence folder name\n",
    "#         if idx < 10:\n",
    "#             video = '00' + str(idx)\n",
    "#         elif idx < 100:\n",
    "#             video = '0' + str(idx)\n",
    "#         else:\n",
    "#             video = str(idx)\n",
    "#         # Getting random sequence of 5 LR frames from the video    \n",
    "#         num_video_frames = len(os.listdir(f\"{self.hr_dir}/000\"))\n",
    "#         rand_frame_id = np.random.randint(2, num_video_frames - 2)\n",
    "#         lr_frame_idx = []\n",
    "#         for i in range(-2, 3):\n",
    "#             id_int = rand_frame_id + i\n",
    "#             if id_int < 10:\n",
    "#                 id_str = '0000000' + str(id_int)\n",
    "#             elif id_int < 100:\n",
    "#                 id_str = '000000' + str(id_int)\n",
    "#             else:\n",
    "#                 id_str = '00000' + str(id_int)\n",
    "#             lr_frame_idx.append(id_str)\n",
    "#         # Actually reading in the images\n",
    "#         hr_frame = torchvision.io.read_image(f\"{self.hr_dir}/{video}/{lr_frame_idx[2]}.png\").to(self.device)\n",
    "#         lr_frames = []\n",
    "#         for v in lr_frame_idx:\n",
    "#             lr_frame = torchvision.io.read_image(f\"{self.lr_dir}/{video}/{v}.png\").to(self.device)\n",
    "#             lr_frames.append(lr_frame)\n",
    "#         lr_frames = torch.stack(lr_frames).permute(1, 0, 2, 3)\n",
    "#         # hr_frame is of size 3x720x1280 (CxHxW)\n",
    "#         # lr_imgs of of size 5x3x180x320 (TxCxHxW)\n",
    "#         # where C=channel, T=time (video sequence)\n",
    "#         return torch.tensor(lr_frames).float(), hr_frame.float()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8859036c19a75c4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:39.143885Z",
     "start_time": "2024-04-16T22:39:38.996306Z"
    }
   },
   "source": [
    "# If using REDS\n",
    "# train_dataset = REDS(train=True, device=device)\n",
    "# test_dataset = REDS(train=False, device=device)\n",
    "\n",
    "# If using moving MNIST : http://www.cs.toronto.edu/~nitish/unsupervised_video/\n",
    "# This will download all the data\n",
    "data = torchvision.datasets.MovingMNIST(root='./', split=None,\n",
    "                                        split_ratio=10, download=True)\n",
    "train_data = data[0:8000].to('cpu')      # 80% for train\n",
    "test_data = data[8000:10001].to('cpu')    # 20% for test\n",
    "\n",
    "def transform_image(image):\n",
    "    # Performs downsampling and then blurring\n",
    "    image = torchvision.transforms.functional.resize(image, (32, 32))\n",
    "    gb = torchvision.transforms.GaussianBlur(kernel_size=(3,3))\n",
    "    image = gb(image)\n",
    "    return image\n",
    "    \n",
    "def get_sample(data, idx):\n",
    "    \"\"\"\n",
    "    Given a dataset (train/test) and a sample idx, a random sequence\n",
    "    of 5 LR frames is computed\n",
    "    \"\"\"\n",
    "    num_video_frames = data[idx].size()[0]\n",
    "    rand_frame_idx = np.random.randint(2, num_video_frames - 2)\n",
    "    # Get HR frame\n",
    "    hr_frame = data[idx][rand_frame_idx].to(device)\n",
    "    # Get LR frames\n",
    "    lr_frames = [data[idx][k] for k in range(rand_frame_idx-2, rand_frame_idx+3)]\n",
    "    lr_frames = [transform_image(frame) for frame in lr_frames]\n",
    "    lr_frames = torch.stack(lr_frames, dim=0).to(device) # permuted to have channel first\n",
    "    return lr_frames, hr_frame\n",
    "\n",
    "lr_frames, hr_frame = get_sample(data, 0)\n",
    "print('Comparing the 5 low-res images to the one high-res one')\n",
    "for f in lr_frames:\n",
    "    pil_fr = torchvision.transforms.ToPILImage()(f)\n",
    "    display(pil_fr)\n",
    "display(torchvision.transforms.ToPILImage()(hr_frame))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the 5 low-res images to the one high-res one\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAyElEQVR4nGNgGFLA4MfOWefr8Cjw+vXhw7dz+IxQYWDIxlTAhGDeYWBxeoTPhMjz9/6+1sdjgjLTnq+X7+EzgoHvrQdeeQbfv2L4HMnAoIhFE4oC5rpXMCYHlxQTpmqYZNXVC1dn/8jCqWD7t42pYha/tHAqsBJjYOA9NwOnPAMDAwPz7H08EBYLFicI2Vg4XPP7gkuzzOlZV3/9MoJxMfzCO+OHBZPrcR+cCjQN/s4wPXjYGLf7WBkYGBgMLuFWAAH8hBQMMwAA5m8z/Uzcu50AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABIElEQVR4nNXRTUvDMBgH8CTrWzpYR6VzDMF60F3GJiLe/P4HQQ/D+QKtGyh2pqVr1820HYmHrqzY5gPsf3kC+ZE8TwLAEQTuq9nvIhAvCfsPpH3t3Y0UMH9Yb0UAyYpsKcFMCPwnT72VtVathxIEMD/jGGtCAE4nN0PqwhpAh3lUlK8z4Qn85xFRugiFgHlLo898Ku4BQJym5QEIY62l5skqrwLyUgJtcDWAIU69aVQB/IvsomJpDsdayOwdfK5ewchhNB77xEAsyqqgks37YtMdx/O3bTPQ9Y5pnagfM483AnxuX/bsfOq4KWgCysX9tUUN8ruioBEgo8NenbaJDNAMeOIEn440kpAAZO43TzJdlYpXq/9vEdhGLBHsHWH+AE/ea90gWcJSAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABJ0lEQVR4nN3SzU/CMBgG8LdjhdlFTRGGovGDbCZGTTh48f+/GK9KohkYEYPLYCFs6z7p8IBjErq78bm9zS/v06QF+A9BAACA202SpjPb2wYyAAAol3fHafj0KADSj8PVPd3QBBWrDckgokfXgEtB/MWTpkQUnJYAIPpt97AvSaV3AISqi9hfllUAMwNCh6Pthhz4vnvFJ075BgAVBU5cnGNCsCyzub8GUV/+4PmAaMfY9dOdqWmugddDbpYPtdObi2ihtaxxUcHYr2KEkokdUxS6BdhIZjOedfioZ4lBRSUN2jgIXl4DMdDOjJN2a/jw7IAQ1Lv3eoWTOJgtxUChyvTdMsJ9hYlBNsbR4NPSQgz5l9sMVuWE8fM6f/PEYJVaFSLB4/3JfAOHpW0GglJWFwAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABJklEQVR4nOXSS0/CQBQF4DMD9BGs2BLTAguNMZpo1BAXLvz/G2NcGIOYKAISSGmhVZrSDjrjBqiY6dqFZ3u/3EdygX8RAgBmbYfgw/W5BBQBwL46VUj3JorzAC2ppV3Fb+UC726oXmpaQbZDEQCmZNGArqu5AHazecReqAzQ5S0KZTOW20G4tyJJe0Eu4O640uBestGaUM7FagcQPU3CdbFkOSYVxelwlAH/MVlPUPYuDuFRa8wyIAbe4n0FCkZFj4RTI9mZ4P6P8WL+OhiXVdYeZWAjn8OZYe1r3VYgB1u2U6832NP925cUqAfnJ9Vt8yHopJAC8+za8TrVkArIAY17/ef+sVHWmRzEbTcahfMaIwCWL/crRACWwSdxHgAAQiD70L/JN6TaawB0fnjcAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABLElEQVR4nO3S3UvCUBgG8OecHduyTSlb2yIiKIMQEr0quulfL7rpIigwVEIoJhLq3BcuG/voIuYUzy6767l9f+97zss5wH8AgACoGBWQ+ecs5QEGQLttCNS8e1oUAUEsCzXF7BaC8UOftXWRcu/AADgfoU4lqVQIiH7TrKPHBxQAKGEInO/CCcno3r6KB04hSK2ZfBFNVycQQpEkGQDIduL6y2L5YF+KWTQZfuXA7jh2BqqNthpYu8LrNAeJaS28DMiGCidSt97i/IjUXvYD4dCzg1PX7c+B38daDxHlHfnoHC/PXj5hJbSmGYdnyuix44MHyF6rdaxo/uDdTrmgdHLdjM0Ji4UUXEAFt2v2kktWBR9EZkjHlsSU7HNwtkgBpoq+tVFZU3Sz88/yA1U0aCLN6fiFAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAABsUlEQVR4nO2Uv0tCURiGX8MIEVq6TgVCQl6JCLo0FGhNbkFK/QlJa0MQtplLf4GQizg0KUJDjuHUUG2pU2QQ1aGW6NdyvmOD/bQrnXPuEnSf6dzDeZ/v+y7nXsDFxeUvEmqzzOrAykHmYEhPkKd3xmUjfT3212UF3m9P9y/9twBg9Mvmu5lLAgBOqKDXAWoAAP+AfEnbdzAfwa0TwWAI2JMVdI3gm1wOw5gGNq7OZBXfGPu4CDuSiV734FpW0DXC0z4uC8iPXyeOJQW2HNKu9Fm7EWYn0HAkmPGh4kiwIJ3uIVDCRjAyrCLw/twKjSLd/CUWxJ2J5jMAj01TWw/b7Z7R2GK06TEvEAl7Kksqrb5VLgsSJKiVXmNtUs9HGCdOPBc3YHHKqAuOBLXSnZ9WmR5NdUEiFzc6qyTxtHr+E4sJ5iSPGmdTDuL+smAR/XgsVdetH7SsbIMEiZJe6U3Gidc5cdKrv0ZHUSsr6PFB5LQEjEeTxOnULNKNRtxioi5IsE0gJSigLqh1voIpAAbxlLogUG0VU2/3WJCGADA+ViVe1RF86ebcoQCm8fsZF5d/xivvKqfNn5pOPAAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:39.147921Z",
     "start_time": "2024-04-16T22:39:39.145411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h_in = 32\n",
    "w_in = 32\n",
    "c_in = 1\n",
    "k = 3\n",
    "s = 1\n",
    "p = 1\n",
    "d = 1\n",
    "\n",
    "conv_h_out = ((h_in + (2*p) - (d*(k-1)) -1) / s) + 1\n",
    "conv_w_out = ((h_in + (2*p) - (d*(k-1)) -1) / s) + 1\n",
    "\n",
    "k = 2\n",
    "s = 2\n",
    "p = 0\n",
    "d = 1\n",
    "\n",
    "pool_h_out = ((conv_h_out + (2*p) - (d*(k-1)) -1) / s) + 1\n",
    "pool_w_out = ((conv_w_out + (2*p) - (d*(k-1)) -1) / s) + 1\n",
    "\n",
    "print(pool_h_out, pool_w_out)"
   ],
   "id": "e495d623189aab6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0 16.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:39.287212Z",
     "start_time": "2024-04-16T22:39:39.148724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recon = reconstructor(device=device)\n",
    "lr_frames, hr_frame = get_sample(train_data, 0)\n",
    "recon = recon(lr_frames.float())\n",
    "torch"
   ],
   "id": "67e24d63612c4837",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'torch' from '/Users/chadw/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "566a961439deb8ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:39.876982Z",
     "start_time": "2024-04-16T22:39:39.287808Z"
    }
   },
   "source": [
    "# Set up loss functions\n",
    "\n",
    "# perceptual_loss\n",
    "vgg = torchvision.models.vgg19(weights='VGG19_Weights.IMAGENET1K_V1').features # removes final classification layer as we don't need it\n",
    "vgg.to(device)\n",
    "vgg.eval() # sets the model to evaluation mode, to not update weights/parameters\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False # don't calculate gradients after forward passes, reduces computation\n",
    "    \n",
    "# A hook function can be used on any nn.module(), like any layer of a neural net\n",
    "# Depending on whether it's a backwards hook (backprop) or forward hook (forward pass),\n",
    "# the inputs and outputs of that nn.module can be accessed for that operation\n",
    "# It's like an event, that each time it happens, the inputs/outputs of that module are saved and then used by vgg_hook\n",
    "vgg_activations = {3:torch.tensor(0),\n",
    "                   8:torch.tensor(0),\n",
    "                   17:torch.tensor(0),\n",
    "                   26:torch.tensor(0),\n",
    "                   35:torch.tensor(0)}\n",
    "def vgg_hook(module, input, output):   \n",
    "    # Get activations at several layers\n",
    "    for layer in [3, 8, 17, 26, 35]: # each of the layers we want\n",
    "        if module == vgg[layer]: \n",
    "            vgg_activations[layer] = output\n",
    "    \n",
    "for layer in [3, 8, 17, 26, 35]:\n",
    "    vgg[layer].register_forward_hook(vgg_hook)\n",
    "\n",
    "def perceptual_loss(generated_img, target_img): # we want a lower value\n",
    "    # with torch.no_grad():\n",
    "    _ = vgg(generated_img)\n",
    "    generated_activations = vgg_activations.copy()\n",
    "    _ = vgg(target_img)\n",
    "    target_activations = vgg_activations.copy()\n",
    "    \n",
    "    # calculate F1 (mean absolute loss (MAE)) for each activation layer\n",
    "    mae_loss = []\n",
    "    for layer in [3, 8, 17, 26, 35]:\n",
    "        mae = F.l1_loss(generated_activations[layer],\n",
    "                        target_activations[layer])\n",
    "        mae_loss.append(mae)\n",
    "        \n",
    "    return sum(mae_loss)\n",
    "\n",
    "# Test if perceptual loss works\n",
    "lr_imgs1, hr_img1 = get_sample(train_data, 20)\n",
    "lr_imgs2, hr_img2 = get_sample(train_data, 30)\n",
    "\n",
    "print(f\"Differing video sequences: {perceptual_loss(hr_img1.expand(3,-1,-1).float(), hr_img2.expand(3,-1,-1).float())}\")\n",
    "print(f\"Same frame: {perceptual_loss(hr_img2.expand(3,-1,-1).float(), hr_img2.expand(3,-1,-1).float())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differing video sequences: 153.77316284179688\n",
      "Same frame: 0.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:39.879220Z",
     "start_time": "2024-04-16T22:39:39.877548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters for training/testing\n",
    "new_model = False\n",
    "num_epochs = 1\n",
    "# pq batches take ~ 1 secs, so 1 epoch ~ 1.5 minutes\n",
    "# mse batches take ~ 1 secs, so 1 epoch ~ 1.5 minutes\n",
    "\n",
    "# To run an entire epoch through takes ~ 3 minutes for both models\n",
    "\n",
    "learning_rate = 0.000001\n",
    "batch_size = 100\n",
    "train = True           # if false, then the models will initialize with best weights (for inference/testing)"
   ],
   "id": "4799eeebab0107b2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:40.985134Z",
     "start_time": "2024-04-16T22:39:39.879884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# Can be either 'mse' or 'pq'\n",
    "loss_function = 'pq'\n",
    "path = f'saved_models/recon_model_{loss_function}.pt'\n",
    "# Initialize Model architecture\n",
    "pq_recon_model = reconstructor(device=device).to(device)\n",
    "if new_model:\n",
    "    # Train/Test \n",
    "    # Mean Square Error/Perceptual Quality\n",
    "    train_mse_loss = []\n",
    "    train_pq_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_pq_loss = []\n",
    "    best_val_loss = float('inf')\n",
    "else:\n",
    "    # Get dictionary of info\n",
    "    old_model = torch.load(path, map_location=device)\n",
    "    # load model with previous weights/parameters\n",
    "    pq_recon_model.load_state_dict(old_model[0])\n",
    "    # update previous lists of loss\n",
    "    if loss_function == 'mse':\n",
    "        train_mse_loss = old_model[1]\n",
    "        test_mse_loss = old_model[2]\n",
    "    else:\n",
    "        train_pq_loss = old_model[1]\n",
    "        test_pq_loss = old_model[2]\n",
    "    best_val_loss = old_model[3]\n",
    "    \n",
    "if train:\n",
    "    start_time = time.time()\n",
    "    prev_batch_time = start_time\n",
    "    mse = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params=pq_recon_model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        pq_recon_model.train()\n",
    "        running_loss = 0\n",
    "        epoch_running_loss = 0\n",
    "        for i in range(len(train_data)):\n",
    "            # Get data sample\n",
    "            lr_imgs, hr_img = get_sample(train_data, i)\n",
    "            lr_imgs = lr_imgs.float()\n",
    "            hr_img = hr_img.float().to(device)\n",
    "            # Do prediction\n",
    "            hr_pred = pq_recon_model(lr_imgs)\n",
    "            # calculate loss\n",
    "            if loss_function == 'mse':\n",
    "                loss = mse(hr_pred, \n",
    "                           hr_img)\n",
    "            else:\n",
    "                loss = perceptual_loss(hr_pred.expand(3,-1,-1), \n",
    "                                       hr_img.expand(3,-1,-1))\n",
    "            # propagate loss through weights to find gradients\n",
    "            # and also adds gradients up\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            epoch_running_loss += loss.item()\n",
    "            del lr_imgs, hr_img, hr_pred\n",
    "            \n",
    "            if i % batch_size == 0 and i != 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                avg_batch_loss = running_loss / batch_size\n",
    "                running_loss = 0\n",
    "                print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "                print(f'Batch: {int((i+1)/batch_size)} / {int(len(train_data)/batch_size)}')\n",
    "                print(f'Avg. Training Loss: {avg_batch_loss:.4f}')\n",
    "                # print(f'W-values: {pq_recon_model.w}')\n",
    "                # for name, param in pq_recon_model.named_parameters():\n",
    "                #     if param.requires_grad and name=='lr.conv3.bias':\n",
    "                #         print(param.data[0:5])\n",
    "                # Calculate batch training time   \n",
    "                curr_time = time.time()\n",
    "                time_diff = (curr_time - prev_batch_time) # in seconds\n",
    "                minutes, seconds = divmod(time_diff, 60)\n",
    "                minutes = int(minutes)\n",
    "                seconds = int(seconds)\n",
    "                print(f'Training Time:')\n",
    "                print(f'{minutes} minutes:{seconds} seconds')\n",
    "                print()\n",
    "                prev_batch_time = curr_time\n",
    "                \n",
    "        avg_epoch_loss = epoch_running_loss / len(train_data)\n",
    "        if loss_function == 'mse':\n",
    "            train_mse_loss.append(avg_epoch_loss)\n",
    "        else:\n",
    "            train_pq_loss.append(avg_epoch_loss)\n",
    "        print(f'Epoch Avg. Training Loss: {avg_epoch_loss:.4f}')\n",
    "            \n",
    "        # Testing\n",
    "        pq_recon_model.eval()\n",
    "        running_loss = 0\n",
    "        epoch_running_loss = 0\n",
    "        for i in range(len(test_data)):\n",
    "            with torch.no_grad():\n",
    "                # Get data sample\n",
    "                lr_imgs, hr_img = get_sample(test_data, i)\n",
    "                hr_img = hr_img.float().to(device)\n",
    "                # Do prediction\n",
    "                hr_pred = pq_recon_model(lr_imgs)\n",
    "                hr_pred.requires_grad = True\n",
    "                # calculate loss\n",
    "                if loss_function == 'mse':\n",
    "                    loss = mse(hr_pred, \n",
    "                               hr_img)\n",
    "                else:\n",
    "                    loss = perceptual_loss(hr_pred.expand(3,-1,-1), \n",
    "                                           hr_img.expand(3,-1,-1))\n",
    "                running_loss += loss.item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                del lr_imgs, hr_img, hr_pred\n",
    "                \n",
    "                if i%batch_size == 0 and i != 0:\n",
    "                    avg_batch_loss = running_loss / batch_size\n",
    "                    running_loss = 0\n",
    "                    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "                    print(f'Batch: {int((i+1)/batch_size)} / {int(len(test_data)/batch_size)}')\n",
    "                    print(f'Avg. Testing Loss: {avg_batch_loss:.4f}')\n",
    "                    # print(f'W-values: {pq_recon_model.w}')\n",
    "                    # for name, param in pq_recon_model.named_parameters():\n",
    "                    #     if param.requires_grad and name=='lr.conv3.bias':\n",
    "                    #         print(param.data[0:5])\n",
    "                    # Calculate batch training time   \n",
    "                    curr_time = time.time()\n",
    "                    time_diff = (curr_time - prev_batch_time) # in seconds\n",
    "                    minutes, seconds = divmod(time_diff, 60)\n",
    "                    minutes = int(minutes)\n",
    "                    seconds = int(seconds)\n",
    "                    print(f'Testing Time:')\n",
    "                    print(f'{minutes} minutes:{seconds} seconds')\n",
    "                    print()\n",
    "                    prev_batch_time = curr_time\n",
    "                    \n",
    "        avg_epoch_loss = epoch_running_loss / len(test_data)\n",
    "        if loss_function == 'mse':\n",
    "            test_mse_loss.append(avg_epoch_loss)\n",
    "        else:\n",
    "            test_pq_loss.append(avg_epoch_loss)\n",
    "        print(f'Epoch Avg. Testing Loss: {avg_epoch_loss:.4f}')\n",
    "        \n",
    "        # Capture results\n",
    "        if avg_epoch_loss < best_val_loss:\n",
    "            # print(avg_running_loss, best_val_loss)\n",
    "            best_val_loss = avg_epoch_loss\n",
    "            save_dict = dict()\n",
    "            save_dict[0] = pq_recon_model.state_dict()\n",
    "            if loss_function == 'mse':\n",
    "                save_dict[1] = train_mse_loss\n",
    "                save_dict[2] = test_mse_loss\n",
    "            else:\n",
    "                save_dict[1] = train_pq_loss\n",
    "                save_dict[2] = test_pq_loss\n",
    "            save_dict[3] = best_val_loss\n",
    "            torch.save(save_dict, path)\n",
    "            \n",
    "        # Calculate epoch training/testing time\n",
    "        if epoch==0:\n",
    "            curr_time = time.time()\n",
    "            prev_time = start_time\n",
    "        else:\n",
    "            prev_time = curr_time\n",
    "            curr_time = time.time()\n",
    "        time_diff = (curr_time - prev_time) # in seconds\n",
    "        minutes, seconds = divmod(time_diff, 60)\n",
    "        minutes = int(minutes)\n",
    "        seconds = int(seconds)\n",
    "        \n",
    "        print(f'Epoch Time:')\n",
    "        print(f'{minutes} minutes:{seconds} seconds')\n",
    "        print()\n",
    "        print(*\"====================\")\n",
    "        print()"
   ],
   "id": "8bcebbe8401c70a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([3, 32, 256])\n",
      "torch.Size([1, 32, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 56\u001B[0m\n\u001B[1;32m     52\u001B[0m     loss \u001B[38;5;241m=\u001B[39m perceptual_loss(hr_pred\u001B[38;5;241m.\u001B[39mexpand(\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), \n\u001B[1;32m     53\u001B[0m                            hr_img\u001B[38;5;241m.\u001B[39mexpand(\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# propagate loss through weights to find gradients\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# and also adds gradients up\u001B[39;00m\n\u001B[0;32m---> 56\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     58\u001B[0m epoch_running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T22:39:40.985942Z",
     "start_time": "2024-04-16T22:39:40.985900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plotting avg. training and testing loss for MSE-trained model\n",
    "plt.plot(train_pq_loss)\n",
    "plt.plot(test_pq_loss)\n",
    "plt.title(\"Pereceptual Loss\")\n",
    "plt.legend(['Train', 'Test'])\n",
    "\n",
    "lr_imgs, hr_img = get_sample(test_data, 100)\n",
    "hr_pred = pq_recon_model(lr_imgs)\n",
    "\n",
    "# mse_loss = mse(hr_pred, hr_img)\n",
    "# perceptual_loss = perceptual_loss(hr_pred, hr_img)\n",
    "# print(f'MSE Loss: {mse_loss:.4f}')\n",
    "# print(f'Perceptual Loss: {perceptual_loss:.4f}')\n",
    "\n",
    "display(torchvision.transforms.ToPILImage()(hr_img))\n",
    "display(torchvision.transforms.ToPILImage()(hr_pred))"
   ],
   "id": "c647af77cc73aacd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# Can be either 'mse' or 'pq'\n",
    "loss_function = 'mse'\n",
    "path = f'saved_models/recon_model_{loss_function}.pt'\n",
    "# Initialize Model architecture\n",
    "mse_recon_model = reconstructor(device=device).to(device)\n",
    "if new_model:\n",
    "    # Train/Test \n",
    "    # Mean Square Error/Perceptual Quality\n",
    "    train_mse_loss = []\n",
    "    train_pq_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_pq_loss = []\n",
    "    best_val_loss = float('inf')\n",
    "else:\n",
    "    # Get dictionary of info\n",
    "    old_model = torch.load(path, map_location=device)\n",
    "    # load model with previous weights/parameters\n",
    "    mse_recon_model.load_state_dict(old_model[0])\n",
    "    # update previous lists of loss\n",
    "    if loss_function == 'mse':\n",
    "        train_mse_loss = old_model[1]\n",
    "        test_mse_loss = old_model[2]\n",
    "    else:\n",
    "        train_pq_loss = old_model[1]\n",
    "        test_pq_loss = old_model[2]\n",
    "    best_val_loss = old_model[3]\n",
    "    \n",
    "if train:\n",
    "    start_time = time.time()\n",
    "    prev_batch_time = start_time\n",
    "    mse = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params=mse_recon_model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        mse_recon_model.train()\n",
    "        running_loss = 0\n",
    "        epoch_running_loss = 0\n",
    "        for i in range(len(train_data)):\n",
    "            # Get data sample\n",
    "            lr_imgs, hr_img = get_sample(train_data, i)\n",
    "            hr_img = hr_img.float().to(device)\n",
    "            # Do prediction\n",
    "            hr_pred = mse_recon_model(lr_imgs)\n",
    "            # calculate loss\n",
    "            if loss_function == 'mse':\n",
    "                loss = mse(hr_pred, \n",
    "                           hr_img)\n",
    "            else:\n",
    "                loss = perceptual_loss(hr_pred.expand(3,-1,-1), \n",
    "                                       hr_img.expand(3,-1,-1))\n",
    "            # propagate loss through weights to find gradients\n",
    "            # and also adds gradients up\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            epoch_running_loss += loss.item()\n",
    "            del lr_imgs, hr_img, hr_pred\n",
    "            \n",
    "            if i % batch_size == 0 and i != 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                avg_batch_loss = running_loss / batch_size\n",
    "                running_loss = 0\n",
    "                print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "                print(f'Batch: {int((i+1)/batch_size)} / {int(len(train_data)/batch_size)}')\n",
    "                print(f'Avg. Training Loss: {avg_batch_loss:.4f}')\n",
    "                # print(f'W-values: {mse_recon_model.w}')\n",
    "                # for name, param in mse_recon_model.named_parameters():\n",
    "                #     if param.requires_grad and name=='lr.conv3.bias':\n",
    "                #         print(param.data[0:5])\n",
    "                # Calculate batch training time   \n",
    "                curr_time = time.time()\n",
    "                time_diff = (curr_time - prev_batch_time) # in seconds\n",
    "                minutes, seconds = divmod(time_diff, 60)\n",
    "                minutes = int(minutes)\n",
    "                seconds = int(seconds)\n",
    "                print(f'Training Time:')\n",
    "                print(f'{minutes} minutes:{seconds} seconds')\n",
    "                print()\n",
    "                prev_batch_time = curr_time\n",
    "                \n",
    "        avg_epoch_loss = epoch_running_loss / len(train_data)\n",
    "        if loss_function == 'mse':\n",
    "            train_mse_loss.append(avg_epoch_loss)\n",
    "        else:\n",
    "            train_pq_loss.append(avg_epoch_loss)\n",
    "        print(f'Epoch Avg. Training Loss: {avg_epoch_loss:.4f}')\n",
    "            \n",
    "        # Testing\n",
    "        mse_recon_model.eval()\n",
    "        running_loss = 0\n",
    "        epoch_running_loss = 0\n",
    "        for i in range(len(test_data)):\n",
    "            with torch.no_grad():\n",
    "                # Get data sample\n",
    "                lr_imgs, hr_img = get_sample(test_data, i)\n",
    "                hr_img = hr_img.float().to(device)\n",
    "                # Do prediction\n",
    "                hr_pred = mse_recon_model(lr_imgs)\n",
    "                hr_pred.requires_grad = True\n",
    "                # calculate loss\n",
    "                if loss_function == 'mse':\n",
    "                    loss = mse(hr_pred, \n",
    "                               hr_img)\n",
    "                else:\n",
    "                    loss = perceptual_loss(hr_pred.expand(3,-1,-1), \n",
    "                                           hr_img.expand(3,-1,-1))\n",
    "                running_loss += loss.item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                del lr_imgs, hr_img, hr_pred\n",
    "                \n",
    "                if i%batch_size == 0 and i != 0:\n",
    "                    avg_batch_loss = running_loss / batch_size\n",
    "                    running_loss = 0\n",
    "                    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "                    print(f'Batch: {int((i+1)/batch_size)} / {int(len(test_data)/batch_size)}')\n",
    "                    print(f'Avg. Testing Loss: {avg_batch_loss:.4f}')\n",
    "                    # print(f'W-values: {mse_recon_model.w}')\n",
    "                    # for name, param in mse_recon_model.named_parameters():\n",
    "                    #     if param.requires_grad and name=='lr.conv3.bias':\n",
    "                    #         print(param.data[0:5])\n",
    "                    # Calculate batch training time   \n",
    "                    curr_time = time.time()\n",
    "                    time_diff = (curr_time - prev_batch_time) # in seconds\n",
    "                    minutes, seconds = divmod(time_diff, 60)\n",
    "                    minutes = int(minutes)\n",
    "                    seconds = int(seconds)\n",
    "                    print(f'Testing Time:')\n",
    "                    print(f'{minutes} minutes:{seconds} seconds')\n",
    "                    print()\n",
    "                    prev_batch_time = curr_time\n",
    "                    \n",
    "        avg_epoch_loss = epoch_running_loss / len(test_data)\n",
    "        if loss_function == 'mse':\n",
    "            test_mse_loss.append(avg_epoch_loss)\n",
    "        else:\n",
    "            test_pq_loss.append(avg_epoch_loss)\n",
    "        print(f'Epoch Avg. Testing Loss: {avg_epoch_loss:.4f}')\n",
    "        \n",
    "        # Capture results\n",
    "        if avg_epoch_loss < best_val_loss:\n",
    "            # print(avg_running_loss, best_val_loss)\n",
    "            best_val_loss = avg_epoch_loss\n",
    "            save_dict = dict()\n",
    "            save_dict[0] = mse_recon_model.state_dict()\n",
    "            if loss_function == 'mse':\n",
    "                save_dict[1] = train_mse_loss\n",
    "                save_dict[2] = test_mse_loss\n",
    "            else:\n",
    "                save_dict[1] = train_pq_loss\n",
    "                save_dict[2] = test_pq_loss\n",
    "            save_dict[3] = best_val_loss\n",
    "            torch.save(save_dict, path)\n",
    "            \n",
    "        # Calculate epoch training/testing time\n",
    "        if epoch==0:\n",
    "            curr_time = time.time()\n",
    "            prev_time = start_time\n",
    "        else:\n",
    "            prev_time = curr_time\n",
    "            curr_time = time.time()\n",
    "        time_diff = (curr_time - prev_time) # in seconds\n",
    "        minutes, seconds = divmod(time_diff, 60)\n",
    "        minutes = int(minutes)\n",
    "        seconds = int(seconds)\n",
    "        \n",
    "        print(f'Epoch Time:')\n",
    "        print(f'{minutes} minutes:{seconds} seconds')\n",
    "        print()\n",
    "        print(*\"====================\")\n",
    "        print()"
   ],
   "id": "3825108fab352e76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting avg. training and testing loss for MSE-trained model\n",
    "plt.plot(train_mse_loss)\n",
    "plt.plot(test_mse_loss)\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend(['Train', 'Test'])\n",
    "\n",
    "lr_imgs, hr_img = get_sample(test_data, 25)\n",
    "hr_pred = mse_recon_model(lr_imgs)\n",
    "\n",
    "# mse_loss = mse(hr_pred, hr_img)\n",
    "# perceptual_loss = perceptual_loss(hr_pred, hr_img)\n",
    "# print(f'MSE Loss: {mse_loss:.4f}')\n",
    "# print(f'Perceptual Loss: {perceptual_loss:.4f}')\n",
    "\n",
    "display(torchvision.transforms.ToPILImage()(hr_img))\n",
    "display(torchvision.transforms.ToPILImage()(hr_pred))"
   ],
   "id": "6d645f1db2480780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # torch.cuda.empty_cache()\n",
    "# # Can be either 'mse' or 'pq'\n",
    "# loss_function = 'pq'\n",
    "# path = f'saved_models/diff_model_{loss_function}.pt'\n",
    "# # Initialize Model architecture\n",
    "# pq_diff_model = diffusion_vsr(s=torch.tensor(0.0008), \n",
    "#                            train_T=torch.tensor(1000), \n",
    "#                            infer_T=torch.tensor(1000),\n",
    "#                               device=device).to(device)\n",
    "# if new_model:\n",
    "#     # Train/Test \n",
    "#     # Mean Square Error/Perceptual Quality\n",
    "#     train_mse_loss = []\n",
    "#     train_pq_loss = []\n",
    "#     test_mse_loss = []\n",
    "#     test_pq_loss = []\n",
    "#     best_val_loss = float('inf')\n",
    "# else:\n",
    "#     # Get dictionary of info\n",
    "#     old_model = torch.load(path, map_location=device)\n",
    "#     # load model with previous weights/parameters\n",
    "#     pq_diff_model.load_state_dict(old_model[0])\n",
    "#     # update previous lists of loss\n",
    "#     if loss_function == 'mse':\n",
    "#         train_mse_loss = old_model[1]\n",
    "#         test_mse_loss = old_model[2]\n",
    "#     else:\n",
    "#         train_pq_loss = old_model[1]\n",
    "#         test_pq_loss = old_model[2]\n",
    "#     best_val_loss = old_model[3]\n",
    "#     \n",
    "# if train:\n",
    "#     start_time = time.time()\n",
    "#     mse = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(params=pq_diff_model.parameters(),\n",
    "#                                  lr=learning_rate)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "#         # Training\n",
    "#         pq_diff_model.train()\n",
    "#         pq_diff_model.calc_train_steps()\n",
    "#         running_loss = 0\n",
    "#         for i in range(len(train_data)):\n",
    "#             lr_imgs, hr_img = get_sample(train_data, i)\n",
    "#             # random t-step to predict\n",
    "#             t = torch.randint(low=0, high=1000, size=(1,)).to(device)\n",
    "#             # hr_img after t steps of noise addition\n",
    "#             hr_t = pq_diff_model.add_noise(hr_img, t)\n",
    "#             # hr_img after t-1 steps of noise addition\n",
    "#             # This is what we want the model to predict (bc we want the reverse process)\n",
    "#             hr_tmin1 = pq_diff_model.add_noise(hr_img, t-1)\n",
    "#             # Now, let's do one pass through the model\n",
    "#             pred_hr_tmin1 = pq_diff_model(hr_img, lr_imgs, t)\n",
    "#             if loss_function == 'mse':\n",
    "#                 loss = mse(pred_hr_tmin1, \n",
    "#                            hr_tmin1)\n",
    "#             else:\n",
    "#                 loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "#                                        hr_tmin1.expand(3,-1,-1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#             del lr_imgs, hr_img, t, hr_t, hr_tmin1, pred_hr_tmin1\n",
    "#             running_loss += loss.item()\n",
    "#             \n",
    "#             if i%batch_size == 0 and i != 0:\n",
    "#                 avg_running_loss = running_loss / batch_size\n",
    "#                 running_loss = 0\n",
    "#                 if loss_function == 'mse':\n",
    "#                     train_mse_loss.append(avg_running_loss)\n",
    "#                 else:\n",
    "#                     train_pq_loss.append(avg_running_loss)\n",
    "#                 print(f'Batch Avg. Training Loss: {avg_running_loss:.4f}')\n",
    "#             \n",
    "#         # Testing\n",
    "#         pq_diff_model.eval()\n",
    "#         pq_diff_model.calc_test_steps()\n",
    "#         running_loss = 0\n",
    "#         for i in range(len(test_data)):\n",
    "#             with torch.no_grad():\n",
    "#                 lr_imgs, hr_img = get_sample(test_data, i)\n",
    "#                 # random t-step to predict\n",
    "#                 t = torch.randint(low=0, high=50, size=(1,)).to('cpu')\n",
    "#                 # hr_img after t steps of noise addition\n",
    "#                 hr_t = pq_diff_model.add_noise(hr_img, t)\n",
    "#                 # hr_img after t-1 steps of noise addition\n",
    "#                 # This is what we want the model to predict (bc we want the reverse process)\n",
    "#                 hr_tmin1 = pq_diff_model.add_noise(hr_img, t-1)\n",
    "#                 # Now, let's do one pass through the model\n",
    "#                 pred_hr_tmin1 = pq_diff_model(hr_t, lr_imgs, t)\n",
    "#                 if loss_function == 'mse':\n",
    "#                     loss = mse(pred_hr_tmin1, hr_tmin1)\n",
    "#                 else:\n",
    "#                     loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "#                                            hr_tmin1.expand(3,-1,-1))\n",
    "#                 running_loss += loss.item()\n",
    "#                 del lr_imgs, hr_img, t, hr_t, hr_tmin1, pred_hr_tmin1\n",
    "#             \n",
    "#             if i%batch_size == 0 and i != 0:\n",
    "#                 avg_running_loss = running_loss / batch_size\n",
    "#                 running_loss = 0\n",
    "#                 if loss_function == 'mse':\n",
    "#                     test_mse_loss.append(avg_running_loss)\n",
    "#                 else:\n",
    "#                     test_pq_loss.append(avg_running_loss)\n",
    "#                 print(f'Batch Avg. Testing Loss: {avg_running_loss:.4f}')\n",
    "#         \n",
    "#                 # Capture results\n",
    "#                 if avg_running_loss < best_val_loss:\n",
    "#                     # print(avg_running_loss, best_val_loss)\n",
    "#                     best_val_loss = avg_running_loss\n",
    "#                     save_dict = dict()\n",
    "#                     save_dict[0] = pq_diff_model.state_dict()\n",
    "#                     if loss_function == 'mse':\n",
    "#                         save_dict[1] = train_mse_loss\n",
    "#                         save_dict[2] = test_mse_loss\n",
    "#                     else:\n",
    "#                         save_dict[1] = train_pq_loss\n",
    "#                         save_dict[2] = test_pq_loss\n",
    "#                     save_dict[3] = best_val_loss\n",
    "#                     torch.save(save_dict, path)\n",
    "#             \n",
    "#         # Calculate epoch training/testing time\n",
    "#         if epoch==0:\n",
    "#             prev_time = start_time\n",
    "#         else:\n",
    "#             prev_time = curr_time    \n",
    "#         curr_time = time.time()\n",
    "#         time_diff = (curr_time - prev_time) # in seconds\n",
    "#         minutes, seconds = divmod(time_diff, 60)\n",
    "#         minutes = int(minutes)\n",
    "#         seconds = int(seconds)\n",
    "#         \n",
    "#         print(f'Time:')\n",
    "#         print(f'{minutes} minutes:{seconds} seconds')\n",
    "#         print()\n",
    "# \n",
    "# # Plotting avg. training and testing loss for MSE-trained model\n",
    "# plt.plot(train_pq_loss)\n",
    "# plt.plot(test_pq_loss)\n",
    "# plt.title(\"Pereceptual Loss\")\n",
    "# plt.legend(['Train', 'Test'])\n",
    "# \n",
    "# ### Testing diffusion model code\n",
    "# \n",
    "# torch.cuda.empty_cache()\n",
    "# # Can be either 'mse' or 'pq'\n",
    "# loss_function = 'mse'\n",
    "# path = f'saved_models/diff_model_{loss_function}.pt'\n",
    "# # Initialize Model architecture\n",
    "# mse_diff_model = diffusion_vsr(s=torch.tensor(0.0008), \n",
    "#                            train_T=torch.tensor(1000), \n",
    "#                            infer_T=torch.tensor(50)).to(device)\n",
    "# if new_model:\n",
    "#     # Train/Test \n",
    "#     # Mean Square Error/Perceptual Quality\n",
    "#     train_mse_loss = []\n",
    "#     train_mse_loss = []\n",
    "#     test_mse_loss = []\n",
    "#     test_mse_loss = []\n",
    "#     best_val_loss = float('inf')\n",
    "# else:\n",
    "#     # Get dictionary of info\n",
    "#     old_model = torch.load(path, map_location=device)\n",
    "#     # load model with previous weights/parameters\n",
    "#     mse_diff_model.load_state_dict(old_model[0])\n",
    "#     # update previous lists of loss\n",
    "#     if loss_function == 'mse':\n",
    "#         train_mse_loss = old_model[1]\n",
    "#         test_mse_loss = old_model[2]\n",
    "#     else:\n",
    "#         train_pq_loss = old_model[1]\n",
    "#         test_pq_loss = old_model[2]\n",
    "#     best_val_loss = old_model[3]\n",
    "#     \n",
    "# if train:\n",
    "#     start_time = time.time()\n",
    "#     mse = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(params=mse_diff_model.parameters(),\n",
    "#                                  lr=learning_rate)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "#         # Training\n",
    "#         mse_diff_model.train()\n",
    "#         mse_diff_model.calc_train_steps()\n",
    "#         running_loss = 0\n",
    "#         for i in range(len(train_data)):\n",
    "#             lr_imgs, hr_img = get_sample(train_data, i)\n",
    "#             # random t-step to predict\n",
    "#             t = torch.randint(low=0, high=1000, size=(1,)).to(device)\n",
    "#             # hr_img after t steps of noise addition\n",
    "#             hr_t = mse_diff_model.add_noise(hr_img, t)\n",
    "#             # hr_img after t-1 steps of noise addition\n",
    "#             # This is what we want the model to predict (bc we want the reverse process)\n",
    "#             hr_tmin1 = mse_diff_model.add_noise(hr_img, t-1)\n",
    "#             # Now, let's do one pass through the model\n",
    "#             pred_hr_tmin1 = mse_diff_model(hr_img, lr_imgs, t)\n",
    "#             if loss_function == 'mse':\n",
    "#                 loss = mse(pred_hr_tmin1, \n",
    "#                            hr_tmin1)\n",
    "#             else:\n",
    "#                 loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "#                                        hr_tmin1.expand(3,-1,-1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#             running_loss += loss.item()\n",
    "#             \n",
    "#             if i%batch_size == 0 and i != 0:\n",
    "#                 avg_running_loss = running_loss / batch_size\n",
    "#                 running_loss = 0\n",
    "#                 if loss_function == 'mse':\n",
    "#                     train_mse_loss.append(avg_running_loss)\n",
    "#                 else:\n",
    "#                     train_pq_loss.append(avg_running_loss)\n",
    "#                 print(f'Batch Avg. Training Loss: {avg_running_loss:.4f}')\n",
    "#             \n",
    "#         # Testing\n",
    "#         mse_diff_model.eval()\n",
    "#         mse_diff_model.calc_test_steps()\n",
    "#         running_loss = 0\n",
    "#         for i in range(len(test_data)):\n",
    "#             with torch.no_grad():\n",
    "#                 lr_imgs, hr_img = get_sample(test_data, i)\n",
    "#                 # random t-step to predict\n",
    "#                 t = torch.randint(low=0, high=50, size=(1,)).to(device)\n",
    "#                 # hr_img after t steps of noise addition\n",
    "#                 hr_t = mse_diff_model.add_noise(hr_img, t)\n",
    "#                 # hr_img after t-1 steps of noise addition\n",
    "#                 # This is what we want the model to predict (bc we want the reverse process)\n",
    "#                 hr_tmin1 = mse_diff_model.add_noise(hr_img, t-1)\n",
    "#                 # Now, let's do one pass through the model\n",
    "#                 pred_hr_tmin1 = mse_diff_model(hr_t, lr_imgs, t)\n",
    "#                 if loss_function == 'mse':\n",
    "#                     loss = mse(pred_hr_tmin1, hr_tmin1)\n",
    "#                 else:\n",
    "#                     loss = perceptual_loss(pred_hr_tmin1.expand(3,-1,-1), \n",
    "#                                            hr_tmin1.expand(3,-1,-1))\n",
    "#                 running_loss += loss.item()\n",
    "#             \n",
    "#             if i%batch_size == 0 and i != 0:\n",
    "#                 avg_running_loss = running_loss / batch_size\n",
    "#                 running_loss = 0\n",
    "#                 if loss_function == 'mse':\n",
    "#                     test_mse_loss.append(avg_running_loss)\n",
    "#                 else:\n",
    "#                     test_pq_loss.append(avg_running_loss)\n",
    "#                 print(f'Batch Avg. Testing Loss: {avg_running_loss:.4f}')\n",
    "#         \n",
    "#                 # Capture results\n",
    "#                 if avg_running_loss < best_val_loss:\n",
    "#                     # print(avg_running_loss, best_val_loss)\n",
    "#                     best_val_loss = avg_running_loss\n",
    "#                     save_dict = dict()\n",
    "#                     save_dict[0] = mse_diff_model.state_dict()\n",
    "#                     if loss_function == 'mse':\n",
    "#                         save_dict[1] = train_mse_loss\n",
    "#                         save_dict[2] = test_mse_loss\n",
    "#                     else:\n",
    "#                         save_dict[1] = train_mse_loss\n",
    "#                         save_dict[2] = test_mse_loss\n",
    "#                     save_dict[3] = best_val_loss\n",
    "#                     torch.save(save_dict, path)\n",
    "#             \n",
    "#         # Calculate epoch training/testing time\n",
    "#         if epoch==0:\n",
    "#             prev_time = start_time\n",
    "#         else:\n",
    "#             prev_time = curr_time    \n",
    "#         curr_time = time.time()\n",
    "#         time_diff = (curr_time - prev_time) # in seconds\n",
    "#         minutes, seconds = divmod(time_diff, 60)\n",
    "#         minutes = int(minutes)\n",
    "#         seconds = int(seconds)\n",
    "#         \n",
    "#         print(f'Time:')\n",
    "#         print(f'{minutes} minutes:{seconds} seconds')\n",
    "#         print()\n",
    "#         \n",
    "# # Plotting avg. training and testing loss for MSE-trained model\n",
    "# plt.plot(train_mse_loss)\n",
    "# plt.plot(test_mse_loss)\n",
    "# plt.title(\"MSE Loss\")\n",
    "# plt.legend(['Train', 'Test'])"
   ],
   "id": "7db34393da7e6e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T23:26:10.579722Z",
     "start_time": "2024-04-16T23:26:10.557912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = np.random.randint(0, 10000)\n",
    "\n",
    "lr_frames, hr_frame = get_sample(data, idx)\n",
    "print('Comparing the 5 low-res images to the one high-res one')\n",
    "for j in range(len(lr_frames)):\n",
    "    f = lr_frames[j]\n",
    "    pil_fr = torchvision.transforms.ToPILImage()(f)\n",
    "    pil_fr.save(f'report/figures/{idx}_{j}_lr.png')\n",
    "    display(pil_fr)\n",
    "pil_hr = torchvision.transforms.ToPILImage()(hr_frame)\n",
    "pil_hr.save(f'report/figures/{idx}_hr.png')\n",
    "display(pil_hr)"
   ],
   "id": "9af5cab6cf256f62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the 5 low-res images to the one high-res one\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA/klEQVR4nGNgGAVDBjCi8Ng4Gb7/QlXAgsxhllb5cf0NqgImiDlC8qLMDAyCpvZKrGhWQEzg0Ja+++Evp5YJ17uvDFhMYJLUV+H4z6ZiKfbyEzc7FgU8mpIfnv0VM9dl5bN3kmXGsIJJWu3PtReCJtZSHxUEJJ4/+otuAoei0NNbLEYumrzM//4x/fuHYQKX0M+Hn9Tc9Vh//P3+9RqqAQxMDAwMrP8/vOU3NuH/x8jz/+7ZF/8xTPjzm0tBwUiclfHT0wtnrn3HDIfv7zRthNQY3zy5fPH6k29YAur7PSEdrruf79+88eTbPzR5SGSxCUkIMH95/ebrX3TpIQIAyNNT28+RWtMAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA8UlEQVR4nO3QzU7CQBQF4DPDVIrQQkpqYVFD/YkGg2FBIEbFlY9PCCEuWEAjBhtNCZURdaBM2Xd4AY1n/eWenAv859eEAAB0Jr+T/YABgFYrRuMYVlm+8zSgAFC6ahwR5Ovda0O5QAHoZ+elH5lx2w1T7aEAsS9zsylzb074RGkAAzSvykcr7+4h++kKP0wUcFiRL1O7++h9kIpLoo0C9IPlK5r3NcYcth0StYLEEseti4yIZRiEWxWItX2b7TgbkYT+4DkNKPA1W9inBVrMrf1ef75nhRiDO4Flrib9pyBOAwIAmmGYZl68BVwqf/gb2QH3rlAL0g9MBgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA80lEQVR4nO2Q30vCUBiG350dMsd2dI5+YQhhpVGjLiKiuu22vzu6ShTBOBBBgjm1HTeZ4nb0ost93hf03n4Pz/vyAf/5LTEAwHLZVG0AOADsN3V3E8AA2M2LgxWY49oGaTAO69uDb4hzq9/LKEA0KsM3fXR5rWb0hr3qUga1h0fveb4iN+wUwnfn5um2UnBKJmXYSlRycnescTZjrQlhSPnu1b3vpnPP98uUYRyf1htVMxlxbWpqw5f8FCILw0Uge7kGcCBsB7xcWmo27HRjCsiUsoqaTz9eW4N8BQcA9F+i4ljKUZr/w8/3TVvwKFrkz38ka52NUodchNwaAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA5klEQVR4nO3Qy07CUBSF4f8cjlJqDYSkClQHxLsTBzrRqW/vDC9RElSMQRAorUUrcoL4AOxOTUxc42/vlSz4zy9FAeCW1TiWgQGguvN9lwE0wOr+YS2rQgNqo14YRBnAAMW90vDJP3JfOoIyQKVq2/Oz48LVxU0qAt8ko+3zLfsVdxeBBvKp9U8rz029VpQr7PLmwYkX56Yfiw8wQDir7wZxUHp8DWWQ5D0/nbg2ao9l4Cx99kbTWXjfn8tg3V95e9f95rU0lgGULuds77bRmmSA7qVxkodGS1xbAV4tcKLOQLr/I/kBxdpPdrnxOqkAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABEUlEQVR4nGNgGAUkAWZWRuwSTBCKRc2GFa8CvmInNnwKWPX1j/zE4wBGia0LhRmZOLDZwsTAwMDAZi866yOzarwdBw4T+Bf1CHAorb1/zoQZuwnMYvsZfGu1ZlwN4MRQwMLAwMDA8O+1YSNf70FzHszAYGFgYGD498nPVOoZU4v8HBxe4Sq68+Hb90fbfLkx5VgYGBgY/n9hPSkss7nn6S9MBUwMDAxMkhFPHogxewj9xWI6CwMDA1+6wEmbVSJ6L/9hUcDEwMBq7L/b4NRtw6Xv/mNXwKLD9Ubqe9KtNT+weYCFgeHf01+hnDarFj7HZgMDCwPDr12scp+P3f2CzQIGBkYGBgZGNub/v7D5YLAAALrASHwD4qgHAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAABe0lEQVR4nO2TPyjEYRyHP65DUkjJrvPvsBoVbjCY6AbThY7JcMlASkkyGCVhMrEJoYxKBklOuk1I7nKdDuccn09nuNzsvZ/B8D7L+77D8/TtfXsBi8VisVj+Ka6ZbL2jQKs06CiwKdU48SuiOihzEthQosmBXhRI6zR/8qXkNQyUSHf5N6gMk6aBFmk+f1gi0w1mfsODzmsBAK6Bq5iYHjIcoFfqzO3aJIlHvzbducWPZBQAULcNoAgnhgPgUKsAgOIViZQSjb81XT+bMwDAVBAvwCQqPf3VhhOMAEBFXHFyrUsin0NGgQ83AMzpjaR3RyL13GES4BAAxHSpZCCSlW7TWY0ZBAJ6nx2GJykxnJI0VRMWTf5md0ZS5ks5Pidcvowuyw0CGD/WD+Qimm/52mfiA6U9y5J0vZDkRbVnl5w28wEAMWnAL3lbDqR4WwGBKOnxk+v35GN7AT5uyL0tkkptVRXiI0iKJCOG95fHHXoSuT9q9g0sFsuf8A0F99NMcSnDtgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "29ef8b53f8d4e777",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
