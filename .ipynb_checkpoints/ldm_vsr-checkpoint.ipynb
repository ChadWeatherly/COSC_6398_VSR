{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Diffusion Model for Video Super Resolution\n",
    "\n",
    "Inspiration gathered from:\n",
    "\n",
    "https://github.com/CompVis/latent-diffusion\n",
    "\n",
    "https://ar5iv.labs.arxiv.org/html/2311.15908"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6be0b271d4abd02"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch as torch\n",
    "import torchvision \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:28:48.899537Z",
     "start_time": "2024-04-09T00:28:46.598425Z"
    }
   },
   "id": "6840a1735dd2c511",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "torchvision.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:26:11.224812Z",
     "start_time": "2024-04-09T00:26:11.221777Z"
    }
   },
   "id": "1de92679892104e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.2+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# datasets with _sharp are the correct/ground truth images\n",
    "# datasets with _blur_bicubic are those that have been blurred and\n",
    "# downsampled using bicubic interpolation\n",
    "datasets = ['train_sharp', 'train_blur_bicubic', 'val_sharp', 'val_blur_bicubic']\n",
    "for set in datasets:\n",
    "    print(set)\n",
    "    if not os.path.isfile(f\"REDS/{set}.zip\"):\n",
    "        # print(\"Downloading\")\n",
    "        cmdlet = f\"python download_REDS.py --{set}\"\n",
    "        print(cmdlet)\n",
    "        os.system(cmdlet)\n",
    "# if not already downloaded, this will download all datasets (takes a while)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:29:09.458148Z",
     "start_time": "2024-04-09T00:29:09.454545Z"
    }
   },
   "id": "e682f168e9ef0017",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sharp\n",
      "train_blur_bicubic\n",
      "val_sharp\n",
      "val_blur_bicubic\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up data into dataset and dataloader\n",
    "# It assumes the project file structure as downloaded from above\n",
    "# Built based on docs: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "class REDS(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.type = 'train' if train else 'test' \n",
    "        if self.type == 'train':\n",
    "            self.hr_dir = \"REDS/train_sharp/train/train_sharp\"\n",
    "            self.lr_dir = \"REDS/train_blur_bicubic/train/train_blur_bicubic/X4\"\n",
    "        else:\n",
    "            self.hr_dir = \"REDS/val_sharp/val/val_sharp\"\n",
    "            self.lr_dir = \"REDS/val_blur_bicubic/val/val_blur_bicubic/X4\"\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.hr_dir)) # training size = 240 videos, testing size = 30 videos\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # each return gives a single HR frame with 5 corresponding LR frames\n",
    "        # the middle LR frame (frame 3) will be the blurred/downsampled version of the HR frame\n",
    "        # the 5 sequential LR frames will be chosen randomly from the given idx-video\n",
    "        \n",
    "        # Getting video sequence folder name\n",
    "        if idx < 10:\n",
    "            video = '00' + str(idx)\n",
    "        elif idx < 100:\n",
    "            video = '0' + str(idx)\n",
    "        # Getting random sequence of 5 LR frames from the video    \n",
    "        num_video_frames = len(os.listdir(f\"{self.hr_dir}/000\"))\n",
    "        rand_frame_id = np.random.randint(2, num_video_frames - 2)\n",
    "        lr_frame_idx = []\n",
    "        for i in range(-2, 3):\n",
    "            id_int = rand_frame_id + i\n",
    "            if id_int < 10:\n",
    "                id_str = '0000000' + str(id_int)\n",
    "            elif id_int < 100:\n",
    "                id_str = '000000' + str(id_int)\n",
    "            else:\n",
    "                id_str = '00000' + str(id_int)\n",
    "            lr_frame_idx.append(id_str)\n",
    "        # Actually reading in the images\n",
    "        hr_frame = torchvision.io.read_image(f\"{self.hr_dir}/{video}/{lr_frame_idx[2]}.png\")\n",
    "        lr_frames = []\n",
    "        for v in lr_frame_idx:\n",
    "            lr_frame = torchvision.io.read_image(f\"{self.lr_dir}/{video}/{v}.png\")\n",
    "            lr_frames.append(lr_frame)\n",
    "            \n",
    "        return lr_frames, hr_frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:29:09.870969Z",
     "start_time": "2024-04-09T00:29:09.866871Z"
    }
   },
   "id": "3783a106bfa2d3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = REDS(train=True)\n",
    "test_dataset = REDS(train=False)\n",
    "# Using the customizable PyTorch dataset allows us to use dataloaders, iterable objects for training/testing that\n",
    "# make it so easy!\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:29:10.261305Z",
     "start_time": "2024-04-09T00:29:10.257490Z"
    }
   },
   "id": "8859036c19a75c4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# Show an example of getting data from either of the datasets\n",
    "# Top is high-res image, bottom is blurred/downsampled low-res image equivalent\n",
    "lr_imgs, hr_img = train_dataset.__getitem__(25)\n",
    "hr_img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:29:10.664876Z",
     "start_time": "2024-04-09T00:29:10.629537Z"
    }
   },
   "id": "789bbd527c4adc87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[154, 157, 159,  ...,  68,  68,  68],\n",
       "         [158, 159, 160,  ...,  68,  68,  68],\n",
       "         [162, 162, 162,  ...,  68,  68,  68],\n",
       "         ...,\n",
       "         [164, 164, 164,  ..., 134, 134, 134],\n",
       "         [164, 164, 164,  ..., 134, 134, 134],\n",
       "         [164, 164, 164,  ..., 134, 134, 134]],\n",
       "\n",
       "        [[161, 162, 164,  ...,  59,  59,  59],\n",
       "         [163, 164, 164,  ...,  59,  59,  59],\n",
       "         [165, 165, 165,  ...,  59,  59,  59],\n",
       "         ...,\n",
       "         [ 61,  61,  61,  ..., 125, 125, 125],\n",
       "         [ 61,  61,  61,  ..., 125, 125, 125],\n",
       "         [ 61,  61,  61,  ..., 125, 125, 125]],\n",
       "\n",
       "        [[162, 163, 165,  ...,  49,  49,  49],\n",
       "         [164, 165, 166,  ...,  49,  49,  49],\n",
       "         [167, 167, 167,  ...,  49,  49,  49],\n",
       "         ...,\n",
       "         [ 47,  47,  47,  ..., 108, 108, 108],\n",
       "         [ 47,  47,  47,  ..., 108, 108, 108],\n",
       "         [ 47,  47,  47,  ..., 108, 108, 108]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(hr_img.permute(1,2,0))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:29:18.393583Z",
     "start_time": "2024-04-09T00:29:18.384818Z"
    }
   },
   "id": "244deb6a828b4eed",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(hr_img\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up loss functions\n",
    "\n",
    "# PSNR\n",
    "\n",
    "# perceptual_loss\n",
    "vgg = torchvision.models.vgg19(weights='VGG19_Weights.IMAGENET1K_V1').features # removes final classification layer as we don't need it\n",
    "vgg.eval() # sets the model to evaluation mode, to not update weights\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:29:16.896351Z",
     "start_time": "2024-04-09T00:29:16.622264Z"
    }
   },
   "id": "566a961439deb8ba",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Set up loss functions\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# PSNR\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# perceptual_loss\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m vgg \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mvgg19(weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVGG19_Weights.IMAGENET1K_V1\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;66;03m# removes final classification layer as we don't need it\u001B[39;00m\n\u001B[0;32m      7\u001B[0m vgg\u001B[38;5;241m.\u001B[39meval() \u001B[38;5;66;03m# sets the model to evaluation mode, to not update weights\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param \u001B[38;5;129;01min\u001B[39;00m vgg\u001B[38;5;241m.\u001B[39mparameters():\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1077611.7500)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg(hr_img.float()).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T20:36:12.203464Z",
     "start_time": "2024-04-08T20:36:11.235522Z"
    }
   },
   "id": "75e48587e4043f82",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 720, 1280])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_img.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T20:33:04.586321Z",
     "start_time": "2024-04-08T20:33:04.583475Z"
    }
   },
   "id": "9ad386e783f15f29",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "28917d9c500efd1b",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
